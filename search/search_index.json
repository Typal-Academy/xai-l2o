{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Explainable AI via Learning to Optimize","text":"<p> Howard Heaton and Samy Wu Fung</p> <p>Summary</p> <p>We fuse optimization-based deep learning models to give explainability with output guarantees and certificates of trustworthiness.</p> <p> Contact Us </p> <p>Key Steps</p> <ul> <li> Create an optimization model with data-driven and analytic terms: \\(\\mathsf{{N}_\\Theta(d) = \\underset{x}{\\text{argmin}}\\  f_\\Theta(x;\\ d)}\\)</li> <li> Identify an optimization algorithm for the model: \\(\\mathsf{x^{k+1} = T_\\Theta(x^k;\\ d)}\\)</li> <li> Train using JFB</li> </ul> <p> Preprint  Slides  Reprint  </p> <p>Abstract</p> <p>Indecipherable black boxes are common in machine learning (ML), but applications increasingly require explainable artificial intelligence (XAI). The core of XAI is to establish transparent and interpretable data-driven algorithms. This work provides concrete tools for XAI in situations where prior knowledge must be encoded and untrustworthy inferences flagged. We use the \"learn to optimize\" (L2O) methodology wherein each inference solves a data-driven optimization problem. Our L2O models are straightforward to implement, directly encode prior knowledge, and yield theoretical guarantees (e.g. satisfaction of constraints). We also propose use of interpretable certificates to verify whether model inferences are trustworthy. Numerical examples are provided in the applications of dictionary-based signal recovery, CT imaging, and arbitrage trading of cryptoassets.</p> <p>Citation</p> <pre><code>@article{heaton2023explainable,\n         title={{Explainable AI via Learning to Optimize}},\n         author={Heaton, Howard and Wu Fung, Samy},\n         journal={Scientific Reports},\n         year={2023},\n         url={https://doi.org/10.1038/s41598-023-36249-3},\n}\n</code></pre>"},{"location":"exp-crypto/","title":"Cryptoasset Trading","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\n\ndevice= 'cuda:0'\n</pre> import torch  device= 'cuda:0'   In\u00a0[\u00a0]: Copied! <pre>def apply_DYS(prox_f, prox_g, grad_h, zeta_init, alpha, \n              fxd_pt_tol=1.0e-12, max_iters=1.0e3, verbose=False):\n    ''' Apply Davis-Yin Splitting to minimize f + g + h.\n\n        Convergence is measured by fixed point residual.\n\n        Preconditions: \n            - Step size is positive\n            - Stopping tolerance is positive\n            - Initialization is a tensor\n\n        Postconditions: \n            - xi and zeta shapes match\n\n        Note: We are unable to directly verify whether alpha &lt; 2 / L, with L the\n              Lipschitz constant of h.\n    '''\n    assert alpha &gt; 0\n    assert fxd_pt_tol &gt; 0\n    assert torch.is_tensor(zeta_init)\n\n    zeta = zeta_init.clone()\n    converge = False\n    iter = 0 \n    while not converge and iter &lt; max_iters:\n        xi = prox_f(zeta, alpha)    \n        psi = prox_g(2.0 * xi - zeta - alpha * grad_h(xi), alpha)\n        zeta = zeta + psi - xi\n\n        fix_pt_res = torch.max(torch.norm(psi - xi, dim=1))        \n        ref_norm = torch.mean(torch.norm(zeta, dim=1))\n        # converge = fix_pt_res &lt;= fxd_pt_tol * ref_norm \n        converge = False\n        iter += 1   \n\n    if verbose and iter == max_iters:\n        print('DYS reached max iteration count.')  \n\n    valid_shape = xi.shape[0] == zeta_init.shape[0]\n    valid_shape = xi.shape[1] == zeta_init.shape[1] and valid_shape\n    assert valid_shape\n\n    return prox_f(zeta, alpha), zeta\n</pre> def apply_DYS(prox_f, prox_g, grad_h, zeta_init, alpha,                fxd_pt_tol=1.0e-12, max_iters=1.0e3, verbose=False):     ''' Apply Davis-Yin Splitting to minimize f + g + h.          Convergence is measured by fixed point residual.          Preconditions:              - Step size is positive             - Stopping tolerance is positive             - Initialization is a tensor          Postconditions:              - xi and zeta shapes match          Note: We are unable to directly verify whether alpha &lt; 2 / L, with L the               Lipschitz constant of h.     '''     assert alpha &gt; 0     assert fxd_pt_tol &gt; 0     assert torch.is_tensor(zeta_init)      zeta = zeta_init.clone()     converge = False     iter = 0      while not converge and iter &lt; max_iters:         xi = prox_f(zeta, alpha)             psi = prox_g(2.0 * xi - zeta - alpha * grad_h(xi), alpha)         zeta = zeta + psi - xi          fix_pt_res = torch.max(torch.norm(psi - xi, dim=1))                 ref_norm = torch.mean(torch.norm(zeta, dim=1))         # converge = fix_pt_res &lt;= fxd_pt_tol * ref_norm          converge = False         iter += 1         if verbose and iter == max_iters:         print('DYS reached max iteration count.')        valid_shape = xi.shape[0] == zeta_init.shape[0]     valid_shape = xi.shape[1] == zeta_init.shape[1] and valid_shape     assert valid_shape      return prox_f(zeta, alpha), zeta In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn \n\nactivation = nn.Sigmoid()\nleaky_relu = nn.LeakyReLU(0.01)\n\ndef grad_neg_U(x, y, p, W, A, psi, num_cfmms):\n    ''' Compute gradient of -U(x,y) with respect to x.\n\n        Preconditions: \n            - Each input is a tensor (other than 'num_cfmms')\n            - Tensor shapes match\n\n        Postconditions:\n            - Output grad shape matches x shape\n\n        Note: The gradient with respect to y is obtained by flipping signs\n    '''\n    for val in [x, y, p, W, A]:\n        assert torch.is_tensor(val)          \n\n    assert x.shape[0] == num_cfmms * p.shape[0]\n    assert x.shape[0] == y.shape[0]\n    assert x.shape[0] == A.shape[1]\n    assert W.shape[0] == A.shape[0]\n    assert W.shape[0] == W.shape[1]\n    \n    AA = torch.mm(A.permute(1,0), A).to(device) \n    WA = torch.mm(W, A).to(device) \n    AWWA = torch.mm(WA.permute(1,0), WA).to(device)\n \n    Sp = torch.cat([p for _ in range(num_cfmms)], dim=0).to(device)\n    cost_term = torch.mm(AA, Sp)\n    \n    \n    risk_term = torch.mm(AWWA, x - y) * psi \n    # risk_term = torch.mm(AWWA, x - y) * torch.mm(A.permute(1,0), psi)\n\n    grad = cost_term + risk_term   \n    return grad.to(device)\n\ndef grad_h(xi, p, W, A, psi, num_cfmms):\n    ''' Compute gradient of h(xi) = - U(x,y) where xi = (v, x, y, z).\n\n        Precondition:\n            - Inputs are tensors and a scalar\n        \n        Postcondition:\n            - Output grad shape matches input xi shape\n\n        Note: The x and y components of gradient are negatives of each other.\n              The indexing of x and y blocks in the tuple xi is done using\n              dummy variables 'q' and 'm'. Also, the v and z blocks of the\n              gradient are automatically zero since there is no dependence in h\n              on these blocks.\n    '''\n    for val in [xi, p, W, A]:\n        assert torch.is_tensor(val) \n\n    q = A.shape[0]\n    m = A.shape[1]\n    grad = xi.clone() \n\n    grad[0:q, :] = torch.zeros(grad[0:q, :].shape)\n    grad[-q:, :] = torch.zeros(grad[0:q, :].shape)\n\n    grad[q:q+m, :] = grad_neg_U(xi[q:q+m, :], xi[q+m:q+2*m, :],p, W, A,\n                                psi, num_cfmms)     \n    grad[q+m:q+2*m, :] = - grad[q:q+m, :]\n\n    valid_shape = grad.shape[0] == xi.shape[0] and grad.shape[1] == xi.shape[1]\n    assert valid_shape\n\n    return grad\n</pre> import torch.nn as nn   activation = nn.Sigmoid() leaky_relu = nn.LeakyReLU(0.01)  def grad_neg_U(x, y, p, W, A, psi, num_cfmms):     ''' Compute gradient of -U(x,y) with respect to x.          Preconditions:              - Each input is a tensor (other than 'num_cfmms')             - Tensor shapes match          Postconditions:             - Output grad shape matches x shape          Note: The gradient with respect to y is obtained by flipping signs     '''     for val in [x, y, p, W, A]:         assert torch.is_tensor(val)                assert x.shape[0] == num_cfmms * p.shape[0]     assert x.shape[0] == y.shape[0]     assert x.shape[0] == A.shape[1]     assert W.shape[0] == A.shape[0]     assert W.shape[0] == W.shape[1]          AA = torch.mm(A.permute(1,0), A).to(device)      WA = torch.mm(W, A).to(device)      AWWA = torch.mm(WA.permute(1,0), WA).to(device)       Sp = torch.cat([p for _ in range(num_cfmms)], dim=0).to(device)     cost_term = torch.mm(AA, Sp)               risk_term = torch.mm(AWWA, x - y) * psi      # risk_term = torch.mm(AWWA, x - y) * torch.mm(A.permute(1,0), psi)      grad = cost_term + risk_term        return grad.to(device)  def grad_h(xi, p, W, A, psi, num_cfmms):     ''' Compute gradient of h(xi) = - U(x,y) where xi = (v, x, y, z).          Precondition:             - Inputs are tensors and a scalar                  Postcondition:             - Output grad shape matches input xi shape          Note: The x and y components of gradient are negatives of each other.               The indexing of x and y blocks in the tuple xi is done using               dummy variables 'q' and 'm'. Also, the v and z blocks of the               gradient are automatically zero since there is no dependence in h               on these blocks.     '''     for val in [xi, p, W, A]:         assert torch.is_tensor(val)       q = A.shape[0]     m = A.shape[1]     grad = xi.clone()       grad[0:q, :] = torch.zeros(grad[0:q, :].shape)     grad[-q:, :] = torch.zeros(grad[0:q, :].shape)      grad[q:q+m, :] = grad_neg_U(xi[q:q+m, :], xi[q+m:q+2*m, :],p, W, A,                                 psi, num_cfmms)          grad[q+m:q+2*m, :] = - grad[q:q+m, :]      valid_shape = grad.shape[0] == xi.shape[0] and grad.shape[1] == xi.shape[1]     assert valid_shape      return grad    In\u00a0[\u00a0]: Copied! <pre>def prox_H(z, w, d, delta, geo_indices, cfmm_type):\n    ''' Project z onto Cartesian product of hyperplanes.\n\n        Inputs:\n                      z - vector to project, z = (z1 z2 ... zj ... zm)\n                      w - vector for scalar product, w = (w1 w2 ... wj ... wm)\n                      d - data vector, d = (d1 d2 ... dj ... dm)\n                  delta - tolerances for projection\n            geo_indices - list of indices for each CFMM\n              cfmm_type - list of booleans identifying which CFMMs are geometric\n        \n        Output:\n            prox - projection of z onto H, the product of hyperplanes\n\n        Preconditions:\n            - All inputs are tensors or lists of tensors\n            - Tensors w, z, and d are same shape\n        Postconditions:\n            - Output prox has same shape as input z\n    '''\n    assert torch.is_tensor(z)     \n    assert torch.is_tensor(w)   \n    assert torch.is_tensor(d)   \n    assert torch.is_tensor(delta)     \n    assert isinstance(geo_indices, list)\n    assert isinstance(cfmm_type, list)\n    assert all([torch.is_tensor(entry) for _, entry in enumerate(cfmm_type)])\n    #assert torch.equal(z.shape, w.shape)\n    #assert torch.equal(z.shape, d.shape)\n\n    zd = z - torch.log(d) \n    wzd = torch.cat([torch.mm(w[idx, :].permute(1,0), zd[idx, :])\n                     for idx in geo_indices], dim=0) \n    wzd_d = wzd - torch.log(1.0 + delta) \n    w_inv = torch.cat([cfmm_type[j] * (torch.norm(w[idx, :]) ** - 2.0)\n                       for j, idx in enumerate(geo_indices)], dim=0) \n    mu = torch.mm(torch.diag(w_inv), wzd_d) \n    Nw = torch.cat([mu[j] * w[idx, :] for j, idx in enumerate(geo_indices)],\n                   dim=0) \n    prox = z - Nw\n\n    # assert torch.equal(z.shape, prox.shape)\n    \n    return prox\n</pre> def prox_H(z, w, d, delta, geo_indices, cfmm_type):     ''' Project z onto Cartesian product of hyperplanes.          Inputs:                       z - vector to project, z = (z1 z2 ... zj ... zm)                       w - vector for scalar product, w = (w1 w2 ... wj ... wm)                       d - data vector, d = (d1 d2 ... dj ... dm)                   delta - tolerances for projection             geo_indices - list of indices for each CFMM               cfmm_type - list of booleans identifying which CFMMs are geometric                  Output:             prox - projection of z onto H, the product of hyperplanes          Preconditions:             - All inputs are tensors or lists of tensors             - Tensors w, z, and d are same shape         Postconditions:             - Output prox has same shape as input z     '''     assert torch.is_tensor(z)          assert torch.is_tensor(w)        assert torch.is_tensor(d)        assert torch.is_tensor(delta)          assert isinstance(geo_indices, list)     assert isinstance(cfmm_type, list)     assert all([torch.is_tensor(entry) for _, entry in enumerate(cfmm_type)])     #assert torch.equal(z.shape, w.shape)     #assert torch.equal(z.shape, d.shape)      zd = z - torch.log(d)      wzd = torch.cat([torch.mm(w[idx, :].permute(1,0), zd[idx, :])                      for idx in geo_indices], dim=0)      wzd_d = wzd - torch.log(1.0 + delta)      w_inv = torch.cat([cfmm_type[j] * (torch.norm(w[idx, :]) ** - 2.0)                        for j, idx in enumerate(geo_indices)], dim=0)      mu = torch.mm(torch.diag(w_inv), wzd_d)      Nw = torch.cat([mu[j] * w[idx, :] for j, idx in enumerate(geo_indices)],                    dim=0)      prox = z - Nw      # assert torch.equal(z.shape, prox.shape)          return prox In\u00a0[\u00a0]: Copied! <pre>def prox_A(v, w, d, delta, geo_indices, cfmm_type):\n    ''' Project v onto Cartesian product of halfspaces.\n\n        Inputs:\n                      v - vector to project, v = (v1 v2 ... vj ... vm)\n                      w - vector for scalar product, w = (w1 w2 ... wj ... wm)\n                      d - data vector, d = (d1 d2 ... dj ... dm)\n                  delta - tolerances for projection\n            geo_indices - list of indices for each CFMM\n              cfmm_type - list of booleans identifying which CFMMs are geometric\n        \n        Output:\n            prox - projection of v onto A, the product of halfspaces\n\n        Note:\n            Here cfmm_type must be negated (i.e. use 1 - type) to get delta_{ja}\n            since we are interested in arithemtic CFMM constraints, *not* \n            geometric CFMM constraints.\n    '''\n    assert torch.is_tensor(v)   \n    assert torch.is_tensor(w)   \n    assert torch.is_tensor(d)   \n    assert torch.is_tensor(delta)        \n    assert isinstance(geo_indices, list)\n    assert isinstance(cfmm_type, list)\n    assert all([torch.is_tensor(entry) for _, entry in enumerate(cfmm_type)])\n\n    # delta = [11, 1]\n    # d = [11, batch size]\n    \n    dd = (delta * d).to(device)\n    #dd = torch.cat([delta[j] * d[idx, :] for j, idx in enumerate(geo_indices)],\n    #               dim=0).to(device)\n    vd = v - dd\n    wvd = torch.cat([torch.mm(w[idx, :].permute(1,0), vd[idx, :])\n                     for idx in geo_indices], dim=0).to(device)\n    w_inv = torch.cat([(1.0 - cfmm_type[j]) * (torch.norm(w[idx, :]) ** - 2.0)\n                       for j, idx in enumerate(geo_indices)],\n                      dim=0).to(device)\n    mu = torch.clamp(torch.mm(torch.diag(w_inv), wvd), max=0.0).to(device)\n    Nw = torch.cat([mu[j] * w[idx, :] for j, idx in enumerate(geo_indices)],\n                   dim=0).to(device)\n    prox = v - Nw\n    return prox\n</pre> def prox_A(v, w, d, delta, geo_indices, cfmm_type):     ''' Project v onto Cartesian product of halfspaces.          Inputs:                       v - vector to project, v = (v1 v2 ... vj ... vm)                       w - vector for scalar product, w = (w1 w2 ... wj ... wm)                       d - data vector, d = (d1 d2 ... dj ... dm)                   delta - tolerances for projection             geo_indices - list of indices for each CFMM               cfmm_type - list of booleans identifying which CFMMs are geometric                  Output:             prox - projection of v onto A, the product of halfspaces          Note:             Here cfmm_type must be negated (i.e. use 1 - type) to get delta_{ja}             since we are interested in arithemtic CFMM constraints, *not*              geometric CFMM constraints.     '''     assert torch.is_tensor(v)        assert torch.is_tensor(w)        assert torch.is_tensor(d)        assert torch.is_tensor(delta)             assert isinstance(geo_indices, list)     assert isinstance(cfmm_type, list)     assert all([torch.is_tensor(entry) for _, entry in enumerate(cfmm_type)])      # delta = [11, 1]     # d = [11, batch size]          dd = (delta * d).to(device)     #dd = torch.cat([delta[j] * d[idx, :] for j, idx in enumerate(geo_indices)],     #               dim=0).to(device)     vd = v - dd     wvd = torch.cat([torch.mm(w[idx, :].permute(1,0), vd[idx, :])                      for idx in geo_indices], dim=0).to(device)     w_inv = torch.cat([(1.0 - cfmm_type[j]) * (torch.norm(w[idx, :]) ** - 2.0)                        for j, idx in enumerate(geo_indices)],                       dim=0).to(device)     mu = torch.clamp(torch.mm(torch.diag(w_inv), wvd), max=0.0).to(device)     Nw = torch.cat([mu[j] * w[idx, :] for j, idx in enumerate(geo_indices)],                    dim=0).to(device)     prox = v - Nw     return prox In\u00a0[\u00a0]: Copied! <pre>def prox_R(v, x, y, A, Gamma):\n    ''' Project (v,x,y) onto constraint set v = A * (Gamma * x - y)\n    '''\n    for val in [v, x, y, A, Gamma]:\n        assert torch.is_tensor(val)\n    # XXX - Check that shapes match up!\n\n    prox_v = v.clone()\n    prox_x = x.clone()\n    prox_y = y.clone()\n    \n    N = torch.cat((torch.mm(Gamma, A), -A), dim=1).to(device)\n    NN = torch.mm(N.permute(1,0), N).to(device)\n    I = torch.diag(torch.ones(NN.shape[0])).to(device)\n    M = torch.linalg.inv(I + NN).to(device)\n    \n    AGv = torch.mm(A.permute(1,0), torch.mm(Gamma, v)).to(device)\n    Av = torch.mm(A.permute(1,0), v).to(device)\n    xy = torch.cat((x + AGv, y - Av), dim=0).to(device)\n    prox_xy = torch.mm(M, xy).to(device)\n    prox_x = prox_xy[:x.shape[0], :]\n    prox_y = prox_xy[x.shape[0]:, :]\n\n    GAx = torch.mm(Gamma, torch.mm(A, prox_x)).to(device)\n    Ay = torch.mm(A, prox_y).to(device)\n    prox_v = GAx - Ay\n\n    return prox_v, prox_x, prox_y\n</pre> def prox_R(v, x, y, A, Gamma):     ''' Project (v,x,y) onto constraint set v = A * (Gamma * x - y)     '''     for val in [v, x, y, A, Gamma]:         assert torch.is_tensor(val)     # XXX - Check that shapes match up!      prox_v = v.clone()     prox_x = x.clone()     prox_y = y.clone()          N = torch.cat((torch.mm(Gamma, A), -A), dim=1).to(device)     NN = torch.mm(N.permute(1,0), N).to(device)     I = torch.diag(torch.ones(NN.shape[0])).to(device)     M = torch.linalg.inv(I + NN).to(device)          AGv = torch.mm(A.permute(1,0), torch.mm(Gamma, v)).to(device)     Av = torch.mm(A.permute(1,0), v).to(device)     xy = torch.cat((x + AGv, y - Av), dim=0).to(device)     prox_xy = torch.mm(M, xy).to(device)     prox_x = prox_xy[:x.shape[0], :]     prox_y = prox_xy[x.shape[0]:, :]      GAx = torch.mm(Gamma, torch.mm(A, prox_x)).to(device)     Ay = torch.mm(A, prox_y).to(device)     prox_v = GAx - Ay      return prox_v, prox_x, prox_y In\u00a0[\u00a0]: Copied! <pre>def prox_P(v, z, d, newton_steps=10):\n    ''' Project (v, z) onto constraint set { (v,z) : ln(v + d) &lt;= z }.\n    ''' \n    assert torch.is_tensor(v)\n    assert torch.is_tensor(z)\n    assert torch.is_tensor(d)\n\n    #print(\"v.shape = \", v.shape)\n    #print(\"z.shape = \", z.shape)\n    #print(\"d.shape = \", d.shape)\n\n\n    with torch.no_grad(): \n        #print(\"v.shape = \", v.shape)\n        #print(\"d.shape = \", d.shape)\n        #print(\"z.shape = \", z.shape)\n        indices = v + d &lt; torch.exp(z)\n                    \n        vv = v[indices]\n        dd = d[indices]\n        pv = torch.max(vv, -dd + 0.5 * torch.abs(dd))\n        zz = z[indices]\n        \n        for _ in range(newton_steps): \n            fv = (pv + dd) * (pv - vv) + torch.log(pv + dd) - zz \n            fpv = 2 * pv + (dd - vv) + ((pv + dd) ** -1.0) \n            pv = torch.max(pv - fv / fpv, -dd + 1.0e-6 * torch.abs(dd)) \n                    \n        prox_v = v.clone()\n        prox_z = z.clone()\n\n        prox_v[indices] = pv.clone()\n        prox_z[indices] = torch.log(pv + dd)\n\n    return prox_v, prox_z\n</pre> def prox_P(v, z, d, newton_steps=10):     ''' Project (v, z) onto constraint set { (v,z) : ln(v + d) &lt;= z }.     '''      assert torch.is_tensor(v)     assert torch.is_tensor(z)     assert torch.is_tensor(d)      #print(\"v.shape = \", v.shape)     #print(\"z.shape = \", z.shape)     #print(\"d.shape = \", d.shape)       with torch.no_grad():          #print(\"v.shape = \", v.shape)         #print(\"d.shape = \", d.shape)         #print(\"z.shape = \", z.shape)         indices = v + d &lt; torch.exp(z)                              vv = v[indices]         dd = d[indices]         pv = torch.max(vv, -dd + 0.5 * torch.abs(dd))         zz = z[indices]                  for _ in range(newton_steps):              fv = (pv + dd) * (pv - vv) + torch.log(pv + dd) - zz              fpv = 2 * pv + (dd - vv) + ((pv + dd) ** -1.0)              pv = torch.max(pv - fv / fpv, -dd + 1.0e-6 * torch.abs(dd))                               prox_v = v.clone()         prox_z = z.clone()          prox_v[indices] = pv.clone()         prox_z[indices] = torch.log(pv + dd)      return prox_v, prox_z In\u00a0[\u00a0]: Copied! <pre>def prox_M(xi, d, w, delta, geo_indices, cfmm_types):\n    ''' Test...\n\n        lsdjf\n    '''\n    assert torch.is_tensor(xi)\n    assert torch.is_tensor(d)\n    assert torch.is_tensor(w)\n    # XXX - preconditions\n\n    q = A.shape[0]\n    m = A.shape[1]\n \n    v_idx = range(q)\n    x_idx = range(q, q+m)\n    y_idx = range(q+m,q+2*m)\n    z_idx = range(q+2*m, xi.shape[0])\n    \n    prox = xi.clone()\n\n    prox_v, prox_z = prox_P(xi[v_idx, :], xi[z_idx, :], d)\n    prox_v = prox_A(prox_v, w, d, delta, geo_indices, cfmm_types)\n\n    prox[v_idx, :] = prox_v\n    prox[z_idx, :] = prox_z\n\n    return prox\n</pre> def prox_M(xi, d, w, delta, geo_indices, cfmm_types):     ''' Test...          lsdjf     '''     assert torch.is_tensor(xi)     assert torch.is_tensor(d)     assert torch.is_tensor(w)     # XXX - preconditions      q = A.shape[0]     m = A.shape[1]       v_idx = range(q)     x_idx = range(q, q+m)     y_idx = range(q+m,q+2*m)     z_idx = range(q+2*m, xi.shape[0])          prox = xi.clone()      prox_v, prox_z = prox_P(xi[v_idx, :], xi[z_idx, :], d)     prox_v = prox_A(prox_v, w, d, delta, geo_indices, cfmm_types)      prox[v_idx, :] = prox_v     prox[z_idx, :] = prox_z      return prox In\u00a0[\u00a0]: Copied! <pre>def prox_f_model(xi, d, w, delta, geo_indices, cfmm_types):\n    '''\n        xi = (v, x, y, z)\n\n        Note: We first project on M. Then we threshold the x and y values.\n        \n        XXX - NEED TO MAKE TESTS FOR THIS\n    '''\n    q = A.shape[0]\n    m = A.shape[1] \n\n    v_idx = range(q)\n    x_idx = range(q, q+m)\n    y_idx = range(q+m,q+2*m)\n    z_idx = range(q+2*m, xi.shape[0])\n\n    prox = prox_M(xi, d, w, delta, geo_indices, cfmm_types)\n    prox[x_idx, :] = torch.clamp(prox[x_idx, :], min=0.0).to(device)\n    prox[y_idx, :] = torch.clamp(prox[y_idx, :], min=0.0).to(device)\n\n    return prox\n</pre> def prox_f_model(xi, d, w, delta, geo_indices, cfmm_types):     '''         xi = (v, x, y, z)          Note: We first project on M. Then we threshold the x and y values.                  XXX - NEED TO MAKE TESTS FOR THIS     '''     q = A.shape[0]     m = A.shape[1]       v_idx = range(q)     x_idx = range(q, q+m)     y_idx = range(q+m,q+2*m)     z_idx = range(q+2*m, xi.shape[0])      prox = prox_M(xi, d, w, delta, geo_indices, cfmm_types)     prox[x_idx, :] = torch.clamp(prox[x_idx, :], min=0.0).to(device)     prox[y_idx, :] = torch.clamp(prox[y_idx, :], min=0.0).to(device)      return prox In\u00a0[\u00a0]: Copied! <pre>def prox_g_model(xi, d, w, delta, geo_indices, cfmm_types, A, Gamma):\n    ''' fff\n    '''\n    q = A.shape[0]\n    m = A.shape[1]\n\n    v_idx = range(q)\n    x_idx = range(q, q+m)\n    y_idx = range(q+m,q+2*m)\n    z_idx = range(q+2*m, xi.shape[0])\n    \n    prox = torch.zeros(xi.shape).to(device)\n    v, x, y = prox_R(xi[v_idx, :], xi[x_idx, :], xi[y_idx, :], A, Gamma)\n\n    prox[v_idx, :] = v\n    prox[x_idx, :] = x\n    prox[y_idx, :] = y\n    prox[z_idx, :] = prox_H(xi[z_idx, :], w, d, delta, geo_indices, cfmm_types)\n    return prox\n</pre> def prox_g_model(xi, d, w, delta, geo_indices, cfmm_types, A, Gamma):     ''' fff     '''     q = A.shape[0]     m = A.shape[1]      v_idx = range(q)     x_idx = range(q, q+m)     y_idx = range(q+m,q+2*m)     z_idx = range(q+2*m, xi.shape[0])          prox = torch.zeros(xi.shape).to(device)     v, x, y = prox_R(xi[v_idx, :], xi[x_idx, :], xi[y_idx, :], A, Gamma)      prox[v_idx, :] = v     prox[x_idx, :] = x     prox[y_idx, :] = y     prox[z_idx, :] = prox_H(xi[z_idx, :], w, d, delta, geo_indices, cfmm_types)     return prox In\u00a0[\u00a0]: Copied! <pre>def get_block_diag(mats):\n    ''' Create block diagonal matrix from list of tensors.\n    '''\n    assert isinstance(mats, list)\n    assert all([torch.is_tensor(entry) for _, entry in enumerate(mats)])    \n    \n    mat = mats[0].to(device)\n    for j, val in enumerate(mats):\n        if j &gt; 0:\n            zeros_top = torch.zeros(mat.shape[0], val.shape[1]).to(device)\n            zeros_bot = torch.zeros(val.shape[0], mat.shape[1]).to(device)\n            mat_top = torch.cat((mat, zeros_top), dim=1).to(device)\n            mat_bot = torch.cat((zeros_bot, val.to(device)), dim=1).to(device)\n            mat = torch.cat((mat_top, mat_bot), dim=0).to(device)\n    return mat\n\ndef get_cfmm_indices(cfmm_sizes):\n    n_cfmms = len(cfmm_sizes)\n    idx_lo = 0\n    cfmm_indices = []\n\n    for j in range(n_cfmms):\n        idx_hi = idx_lo + cfmm_sizes[j]\n        cfmm_indices.append(range(idx_lo,idx_hi))\n        idx_lo = idx_hi\n\n    n_size = sum(cfmm_sizes)\n    assert idx_hi == n_size\n\n    return cfmm_indices\n</pre> def get_block_diag(mats):     ''' Create block diagonal matrix from list of tensors.     '''     assert isinstance(mats, list)     assert all([torch.is_tensor(entry) for _, entry in enumerate(mats)])              mat = mats[0].to(device)     for j, val in enumerate(mats):         if j &gt; 0:             zeros_top = torch.zeros(mat.shape[0], val.shape[1]).to(device)             zeros_bot = torch.zeros(val.shape[0], mat.shape[1]).to(device)             mat_top = torch.cat((mat, zeros_top), dim=1).to(device)             mat_bot = torch.cat((zeros_bot, val.to(device)), dim=1).to(device)             mat = torch.cat((mat_top, mat_bot), dim=0).to(device)     return mat  def get_cfmm_indices(cfmm_sizes):     n_cfmms = len(cfmm_sizes)     idx_lo = 0     cfmm_indices = []      for j in range(n_cfmms):         idx_hi = idx_lo + cfmm_sizes[j]         cfmm_indices.append(range(idx_lo,idx_hi))         idx_lo = idx_hi      n_size = sum(cfmm_sizes)     assert idx_hi == n_size      return cfmm_indices   In\u00a0[\u00a0]: Copied! <pre>#@title\n\nimport random\nimport numpy as np\n\n\ntorch.manual_seed(42)\nrandom.seed(42)\n\n\ndef test_apply_DYS():\n    ''' Apply DYS to toy problem and compare output to true solution.\n\n        Here we consider the 1D problem with \n\n        f(xi) = indicator for [1, 10], g(xi) = -6 * x, h(xi) = x^2 + 9.\n\n        Combining these gives the overall problem\n\n        min (x - 3)^2   s.t.   1 &lt;= x &lt;= 10,\n\n        and xi is initialized to 15.\n    '''\n    tol_err = 1.0e-3\n\n    def prox_f_toy(xi, scalar):\n        return torch.clamp(xi, min=1.0, max=10.0)\n\n    def prox_g_toy(xi, scalar):\n        return xi + 6.0 * scalar\n\n    def grad_h_toy(xi):\n        return 2.0 * xi\n    \n    L = 2.0\n    alpha = 0.5 / L\n    xi_init = 15.0 * torch.ones(1, 1)\n    xi_opt = 3.0 * torch.ones(1, 1)\n    xi_sol, _ = apply_DYS(prox_f_toy, prox_g_toy, grad_h_toy, xi_init, alpha)\n    err = torch.norm(xi_sol - xi_opt)  \n    assert err &lt;= tol_err\n\n\ndef test_prox_H():\n    ''' Check if projection of batch of vectors is in hyperplane is feasible.\n\n        Proximal is feasible if linear equation is satisfied (to numerical\n        precision) for CFMMs that are defined using weighted geometric means.\n        For the remaining arithmetic means, the proximal should give apply the\n        identity operation.\n    '''\n    n_batches = 20\n    n_cfmms = 10\n    tol_feas = 1.0e-6\n    cfmm_sizes = [random.randint(2, 10) for _ in range(n_cfmms)]\n    cfmm_indices = get_cfmm_indices(cfmm_sizes)\n    n_size = sum(cfmm_sizes)\n\n    v = torch.randn(n_size, n_batches)\n    w = torch.rand(n_size, 1)\n    d = torch.rand(n_size, 1)\n    delta = 0.05 * torch.rand(n_cfmms, 1) \n    cfmm_types = [torch.randint(2, (1,)) for _ in range(n_cfmms)]\n    \n    prox = prox_H(v, w, d, delta, cfmm_indices, cfmm_types)  \n\n    feasible = []\n    for j, idx in enumerate(cfmm_indices):\n        cfmm_geometric = bool(cfmm_types[j].numpy()) \n        if cfmm_geometric:\n            wt = w[idx, :].permute(1,0)\n\n            b = torch.log(1 + delta[j]) + torch.mm(wt, torch.log(d[idx, :])) \n            res = torch.mm(wt, prox[idx, :]) - b \n            rel_res = torch.mean(res / torch.norm(prox[idx, :], dim=0))  \n            feasible.append(bool(rel_res &lt;= tol_feas))\n        else:\n            rel_res = torch.mean(torch.norm(prox[idx, :] - v[idx, :], dim=0)) \n            feasible.append(bool(rel_res &lt;= tol_feas)) \n\n    assert all(feasible) \n\ndef test_prox_A():\n    ''' Check if projection of batch of vectors is onto halfspace is feasible.\n\n        Proximal is feasible if linear inequality is satisfied (to numerical\n        precision) for CFMMs that are defined using weighted arithmetic means.\n        For the remaining geometric means, the proximal should give apply the\n        identity operation.\n    '''\n    n_batches = 20\n    n_cfmms = 10\n    tol_feas = 1.0e-6\n    cfmm_sizes = [random.randint(2, 10) for _ in range(n_cfmms)]\n    cfmm_indices = get_cfmm_indices(cfmm_sizes)\n    n_size = sum(cfmm_sizes)\n\n    v = torch.randn(n_size, n_batches).to(device)\n    w = torch.rand(n_size, 1).to(device)\n    d = torch.rand(n_size, 1).to(device)\n    delta = 0.05 * torch.rand(n_cfmms, 1).to(device)\n    cfmm_types = [torch.randint(2, (1,)).to(device) for _ in range(n_cfmms)]\n    \n    prox = prox_A(v, w, d, delta, cfmm_indices, cfmm_types)  \n\n    feasible = []\n    for j, idx in enumerate(cfmm_indices):\n        cfmm_arithmetic = not bool(cfmm_types[j].to('cpu').numpy()) \n        if cfmm_arithmetic:\n            wt = w[idx, :].permute(1,0)\n\n            b = delta[j] * torch.mm(wt, d[idx, :]) \n            res = torch.clamp(torch.mm(wt, prox[idx, :]) - b, max=0.0) \n            rel_res = torch.mean(res / torch.norm(prox[idx, :], dim=0))  \n            feasible.append(bool(rel_res &lt;= tol_feas))\n        else:\n            rel_res = torch.mean(torch.norm(prox[idx, :] - v[idx, :], dim=0)) \n            feasible.append(bool(rel_res &lt;= tol_feas)) \n\n    assert all(feasible) \n\n\ndef test_prox_R():\n    ''' Check if projection onto {v = A * (Gamma * x - y)} is feasible.\n\n        This test requires the creation of a few mock CFMMs, and so matrices\n        A from the global coordinates to the local CFMM coordinates are needed.\n        First the number of tokens in each CFMM is randomly chosen between 2 and\n        the global size. Having this, a binary matrix A is constructed using\n        Bernoulli distributions (violating actual A structure, but close). \n    '''\n    tol_feas = 1.0e-6    \n    n_batches = 20\n    n_cfmms = 10\n    n_tokens = 10\n    cfmm_sizes = [random.randint(2, n_tokens) for _ in range(n_cfmms)]\n    cfmm_indices = get_cfmm_indices(cfmm_sizes)\n    n_size = sum(cfmm_sizes)\n\n    B = [torch.rand(cfmm_sizes[j], n_tokens) for j in range(n_cfmms)] \n    A = [torch.bernoulli(B[j]).to(device) for j in range(n_cfmms)]\n    A = get_block_diag(A).to(device)\n\n    Gamma = [0.97 * torch.eye(cfmm_sizes[j]) for j in range(n_cfmms)]  \n    Gamma = get_block_diag(Gamma).to(device) \n\n    v = torch.randn(n_size, n_batches).to(device)\n    x = torch.randn(n_cfmms * n_tokens, n_batches).to(device)  \n    y = torch.randn(n_cfmms * n_tokens, n_batches).to(device)\n\n    prox_v, prox_x, prox_y = prox_R(v, x, y, A, Gamma)\n\n    GAx = torch.mm(Gamma, torch.mm(A, prox_x))\n    Ay = torch.mm(A,prox_y)\n    res = torch.norm(prox_v - (GAx - Ay), dim=0)\n    rel_res = torch.mean(res / torch.norm(prox_v, dim=0))\n    feasible = bool(rel_res &lt;= tol_feas)\n \n    assert feasible\n\n\ndef test_prox_P():\n    ''' Check whether projection satisfies optimality condition.\n\n        The projection is a solution...\n    '''\n    n_batches = 40\n    n_size = 7\n    tol_feas = 1.0e-2\n    \n    v = torch.randn(n_size, n_batches)\n    z = torch.randn(n_size, n_batches)\n    d = torch.randn(n_size, n_batches)\n\n    prox_v, prox_z = prox_P(v, z, d)\n\n    indices = v + d &lt; torch.exp(z) \n    pv = prox_v[indices]\n    vv = v[indices]\n    zz = z[indices]\n    dd = d[indices]\n    \n    res = (pv + dd) * (pv - vv) + torch.log(pv + dd) - zz\n    res = torch.mean(torch.abs(res))\n    feasible = bool(res &lt;= tol_feas)\n \n    assert feasible\n</pre> #@title  import random import numpy as np   torch.manual_seed(42) random.seed(42)   def test_apply_DYS():     ''' Apply DYS to toy problem and compare output to true solution.          Here we consider the 1D problem with           f(xi) = indicator for [1, 10], g(xi) = -6 * x, h(xi) = x^2 + 9.          Combining these gives the overall problem          min (x - 3)^2   s.t.   1 &lt;= x &lt;= 10,          and xi is initialized to 15.     '''     tol_err = 1.0e-3      def prox_f_toy(xi, scalar):         return torch.clamp(xi, min=1.0, max=10.0)      def prox_g_toy(xi, scalar):         return xi + 6.0 * scalar      def grad_h_toy(xi):         return 2.0 * xi          L = 2.0     alpha = 0.5 / L     xi_init = 15.0 * torch.ones(1, 1)     xi_opt = 3.0 * torch.ones(1, 1)     xi_sol, _ = apply_DYS(prox_f_toy, prox_g_toy, grad_h_toy, xi_init, alpha)     err = torch.norm(xi_sol - xi_opt)       assert err &lt;= tol_err   def test_prox_H():     ''' Check if projection of batch of vectors is in hyperplane is feasible.          Proximal is feasible if linear equation is satisfied (to numerical         precision) for CFMMs that are defined using weighted geometric means.         For the remaining arithmetic means, the proximal should give apply the         identity operation.     '''     n_batches = 20     n_cfmms = 10     tol_feas = 1.0e-6     cfmm_sizes = [random.randint(2, 10) for _ in range(n_cfmms)]     cfmm_indices = get_cfmm_indices(cfmm_sizes)     n_size = sum(cfmm_sizes)      v = torch.randn(n_size, n_batches)     w = torch.rand(n_size, 1)     d = torch.rand(n_size, 1)     delta = 0.05 * torch.rand(n_cfmms, 1)      cfmm_types = [torch.randint(2, (1,)) for _ in range(n_cfmms)]          prox = prox_H(v, w, d, delta, cfmm_indices, cfmm_types)        feasible = []     for j, idx in enumerate(cfmm_indices):         cfmm_geometric = bool(cfmm_types[j].numpy())          if cfmm_geometric:             wt = w[idx, :].permute(1,0)              b = torch.log(1 + delta[j]) + torch.mm(wt, torch.log(d[idx, :]))              res = torch.mm(wt, prox[idx, :]) - b              rel_res = torch.mean(res / torch.norm(prox[idx, :], dim=0))               feasible.append(bool(rel_res &lt;= tol_feas))         else:             rel_res = torch.mean(torch.norm(prox[idx, :] - v[idx, :], dim=0))              feasible.append(bool(rel_res &lt;= tol_feas))       assert all(feasible)   def test_prox_A():     ''' Check if projection of batch of vectors is onto halfspace is feasible.          Proximal is feasible if linear inequality is satisfied (to numerical         precision) for CFMMs that are defined using weighted arithmetic means.         For the remaining geometric means, the proximal should give apply the         identity operation.     '''     n_batches = 20     n_cfmms = 10     tol_feas = 1.0e-6     cfmm_sizes = [random.randint(2, 10) for _ in range(n_cfmms)]     cfmm_indices = get_cfmm_indices(cfmm_sizes)     n_size = sum(cfmm_sizes)      v = torch.randn(n_size, n_batches).to(device)     w = torch.rand(n_size, 1).to(device)     d = torch.rand(n_size, 1).to(device)     delta = 0.05 * torch.rand(n_cfmms, 1).to(device)     cfmm_types = [torch.randint(2, (1,)).to(device) for _ in range(n_cfmms)]          prox = prox_A(v, w, d, delta, cfmm_indices, cfmm_types)        feasible = []     for j, idx in enumerate(cfmm_indices):         cfmm_arithmetic = not bool(cfmm_types[j].to('cpu').numpy())          if cfmm_arithmetic:             wt = w[idx, :].permute(1,0)              b = delta[j] * torch.mm(wt, d[idx, :])              res = torch.clamp(torch.mm(wt, prox[idx, :]) - b, max=0.0)              rel_res = torch.mean(res / torch.norm(prox[idx, :], dim=0))               feasible.append(bool(rel_res &lt;= tol_feas))         else:             rel_res = torch.mean(torch.norm(prox[idx, :] - v[idx, :], dim=0))              feasible.append(bool(rel_res &lt;= tol_feas))       assert all(feasible)    def test_prox_R():     ''' Check if projection onto {v = A * (Gamma * x - y)} is feasible.          This test requires the creation of a few mock CFMMs, and so matrices         A from the global coordinates to the local CFMM coordinates are needed.         First the number of tokens in each CFMM is randomly chosen between 2 and         the global size. Having this, a binary matrix A is constructed using         Bernoulli distributions (violating actual A structure, but close).      '''     tol_feas = 1.0e-6         n_batches = 20     n_cfmms = 10     n_tokens = 10     cfmm_sizes = [random.randint(2, n_tokens) for _ in range(n_cfmms)]     cfmm_indices = get_cfmm_indices(cfmm_sizes)     n_size = sum(cfmm_sizes)      B = [torch.rand(cfmm_sizes[j], n_tokens) for j in range(n_cfmms)]      A = [torch.bernoulli(B[j]).to(device) for j in range(n_cfmms)]     A = get_block_diag(A).to(device)      Gamma = [0.97 * torch.eye(cfmm_sizes[j]) for j in range(n_cfmms)]       Gamma = get_block_diag(Gamma).to(device)       v = torch.randn(n_size, n_batches).to(device)     x = torch.randn(n_cfmms * n_tokens, n_batches).to(device)       y = torch.randn(n_cfmms * n_tokens, n_batches).to(device)      prox_v, prox_x, prox_y = prox_R(v, x, y, A, Gamma)      GAx = torch.mm(Gamma, torch.mm(A, prox_x))     Ay = torch.mm(A,prox_y)     res = torch.norm(prox_v - (GAx - Ay), dim=0)     rel_res = torch.mean(res / torch.norm(prox_v, dim=0))     feasible = bool(rel_res &lt;= tol_feas)       assert feasible   def test_prox_P():     ''' Check whether projection satisfies optimality condition.          The projection is a solution...     '''     n_batches = 40     n_size = 7     tol_feas = 1.0e-2          v = torch.randn(n_size, n_batches)     z = torch.randn(n_size, n_batches)     d = torch.randn(n_size, n_batches)      prox_v, prox_z = prox_P(v, z, d)      indices = v + d &lt; torch.exp(z)      pv = prox_v[indices]     vv = v[indices]     zz = z[indices]     dd = d[indices]          res = (pv + dd) * (pv - vv) + torch.log(pv + dd) - zz     res = torch.mean(torch.abs(res))     feasible = bool(res &lt;= tol_feas)       assert feasible In\u00a0[\u00a0]: Copied! <pre>#@title\n\ntest_apply_DYS()\ntest_prox_A()\ntest_prox_H()\ntest_prox_P()\ntest_prox_R()\nprint('All Pytests pass.')\n</pre> #@title  test_apply_DYS() test_prox_A() test_prox_H() test_prox_P() test_prox_R() print('All Pytests pass.') <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-13-534319815045&gt; in &lt;module&gt;()\n      2 \n      3 test_apply_DYS()\n----&gt; 4 test_prox_A()\n      5 test_prox_H()\n      6 test_prox_P()\n\n&lt;ipython-input-12-4684df0a2ab5&gt; in test_prox_A()\n    102     cfmm_types = [torch.randint(2, (1,)).to(device) for _ in range(n_cfmms)]\n    103 \n--&gt; 104     prox = prox_A(v, w, d, delta, cfmm_indices, cfmm_types)\n    105 \n    106     feasible = []\n\n&lt;ipython-input-5-88ac0cae8cc8&gt; in prox_A(v, w, d, delta, geo_indices, cfmm_type)\n     29     # d = [11, batch size]\n     30 \n---&gt; 31     dd = (delta * d).to(device)\n     32     #dd = torch.cat([delta[j] * d[idx, :] for j, idx in enumerate(geo_indices)],\n     33     #               dim=0).to(device)\n\nRuntimeError: The size of tensor a (10) must match the size of tensor b (49) at non-singleton dimension 0</pre> In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\n\nclass CfmmTradeModel(nn.Module):\n    ''' Implicit L2O model for trading cryptoassets.\n    '''\n    def __init__(self, w, A, p, cfmm_type, delta, Gamma, geo_indices):\n        super(CfmmTradeModel, self).__init__()\n        self.A = A.to(device)\n        self.AA = torch.mm(self.A.permute(1, 0), self.A)\n        self.W = nn.Parameter(torch.rand(A.shape[0], A.shape[0]).to(device))\n\n        self.mat1 = nn.Parameter(torch.randn(A.shape[0], A.shape[0]).to(device))        \n        self.mat2 = nn.Parameter(torch.randn(A.shape[0], A.shape[0]).to(device))\n        self.bias1 = nn.Parameter(torch.randn(A.shape[0],1).to(device))\n        self.bias2 = nn.Parameter(torch.randn(A.shape[0],1).to(device))\n        self.leaky_relu = nn.LeakyReLU(0.1)\n        self.tanh = nn.Tanh()\n        self.relu = nn.ReLU()\n        \n        self.w = w.to(device)\n        # self.delta = delta.to(device)\n        #self.delta = torch.ones(delta.shape).to(device)\n        #self.delta_scale = nn.Parameter(-3 * torch.ones(1).to(device))\n\n        self.delta = nn.Parameter(-3 * torch.ones(delta.shape).to(device))\n\n        self.L = None\n        self.psi_mean = 0\n        self.cfmm_types = cfmm_types\n        self.Gamma = Gamma.to(device)\n        self.p = p.to(device)\n        self.geo_indices = geo_indices\n        \n\n        self.local_size = self.A.shape[0]\n        self.global_size = self.A.shape[1] \n\n\n    def device(self) -&gt; str:\n        return next(self.parameters()).data.device\n\n    def psi(self, d, tol=1.0e-1):\n        ''' Give weight for risk term in utility\n\n            Note: Since phi is used in the utility, we must ensure its output\n                  does not result in a trade transacted with a coin that is not\n                  available for trade on a given CFMM. This is accounted for\n                  by mapping psi from global coordinates to local coordinates,\n                  and then back to global coordinates via A and its transpose.\n                  This process \"zeros out\" any token values that are \"invalid.\"\n        '''\n        sigmoid = nn.Sigmoid()\n        risk_weight = torch.mm(self.mat1, d) + self.bias1\n        risk_weight = self.tanh(torch.mm(self.mat2, risk_weight) + self.bias2) \n        return sigmoid(risk_weight).to(device)\n\n    def zero_weights(self):       \n        n_tokens = int(self.A.shape[1] / len(self.cfmm_types))\n        W_zero = torch.zeros(self.W.data.shape).to(device)          \n\n        idx_lo = 0\n        for j, n_tokens_loc in enumerate(self.geo_indices):\n            idx_hi = idx_lo + len(n_tokens_loc)\n            W_zero[idx_lo:idx_hi, idx_lo:idx_hi] = self.W.data[idx_lo:idx_hi,\n                                                               idx_lo:idx_hi]\n            idx_lo = idx_hi\n                \n        self.W.data = W_zero\n\n\n    def forward(self, d, fxd_pt_tol=1.0e-3, max_iters=int(2.0e3), \n                verbose=False, return_obj=False, analytic=False):\n        ''' Apply Davis-Yin Splitting to minimize f + g + h\n\n            Convergence is measured by fixed point residual.\n\n            XXX - Force L to be actual Lipschitz constant\n\n            Note: Assumes input d = [batch_size, reserves_size]\n        '''\n        assert torch.is_tensor(d) \n\n        self.zero_weights()\n\n        # local coordinate sizes\n        v = torch.zeros(self.local_size, d.shape[1]).to(device)\n        z = torch.zeros(self.local_size, d.shape[1]).to(device)\n\n        # global coordinate sizes\n        x = torch.zeros(self.global_size, d.shape[1]).to(device)  \n        y = torch.zeros(self.global_size, d.shape[1]).to(device) \n        \n        zeta = torch.cat((v, x, y, z), dim=0)\n\n        # delta = torch.exp(self.delta_scale * self.delta)\n        delta = torch.exp(self.delta)\n\n        def prox_f_m(xi, alpha=None): \n            return prox_f_model(xi, d, self.w, delta, self.geo_indices,\n                                self.cfmm_types)\n\n        def prox_g_m(xi, alpha=None):\n            return prox_g_model(xi, d, self.w, delta, self.geo_indices,\n                                self.cfmm_types, self.A,\n                                self.Gamma)   \n\n        # psi = self.psi(d) if not analytic else 0.0 * self.psi(d)\n        # self.psi_mean = torch.mean(psi).detach().to('cpu').numpy()\n        \n        psi = torch.ones(1).to(device) if not analytic else torch.zeros(1).to(device)\n\n\n        W = torch.abs(self.W)\n        # W = torch.zeros(self.W.shape).to(device)\n        # if not analytic:\n        #     W[self.W != 0] = torch.abs(self.W[self.W != 0])\n        \n        # W = torch.exp(self.W) if not analytic else torch.zeros(self.W.shape).to(device)     \n\n        def grad_h_m(xi):  \n            return grad_h(xi, self.p, W, self.A, \n                          psi,\n                          len(self.cfmm_types)) \n        \n\n        WA = torch.mm(W, self.A).to(device)\n        AWWA = torch.mm(WA.permute(1,0), WA).to(device)\n\n        # self.L = 1.0e-5 + torch.linalg.matrix_norm(AWWA, ord=2)\n        \n        # self.psi_scaling =  if not analytic else 1.0\n        self.L = 1.0e-1 + torch.norm(AWWA) * torch.max(psi)\n        alpha = 1.99 / self.L  \n\n        with torch.no_grad(): \n            # Bound singular values to unity and zero out off block diagonals\n            # self.zero_weights()\n            # u, s, v = torch.svd(self.W.data)\n            # s = s / s[0]\n            # #s[s &gt; 1.0] = torch.ones(s[s &gt; 1].shape).to(device)\n            # self.W.data = torch.mm(torch.mm(u, torch.diag(s)), v.t())\n\n            #alpha = min(1.0e2, 0.99 / (float(torch.max(psi_val).cpu() * self.L)))\n            # alpha = min(1.0e2, 0.99 / self.L)\n\n            # print('alpha = ', alpha)\n     \n            xi, zeta = apply_DYS(prox_f_m, prox_g_m, grad_h_m, zeta, alpha, \n                                fxd_pt_tol=fxd_pt_tol, max_iters=max_iters,\n                                verbose=verbose)\n\n        # Attach gradients for final iterations\n        xi, _ = apply_DYS(prox_f_m, prox_g_m, grad_h_m, zeta, alpha, \n                          fxd_pt_tol=fxd_pt_tol, max_iters=1,\n                          verbose=verbose)        \n        q = A.shape[0]\n        m = A.shape[1]\n\n        x_idx = range(q, q+m)\n        y_idx = range(q+m,q+2*m)        \n        x = xi[x_idx, :]\n        y = xi[y_idx, :]\n        \n        # Post conditions\n        tender_nonnegative  = (x &gt;= 0).to('cpu').numpy().all()\n        receive_nonnegative = (y &gt;= 0).to('cpu').numpy().all()\n        assert tender_nonnegative\n        assert receive_nonnegative\n\n        check_violation(x, y, d, self.w, self.A, self.Gamma, geo_indices,\n                        self.cfmm_types, verbose=verbose,\n                        tol_err=1.0e-1)\n\n        return x.permute(1,0), y.permute(1,0)\n</pre> import torch.nn as nn  class CfmmTradeModel(nn.Module):     ''' Implicit L2O model for trading cryptoassets.     '''     def __init__(self, w, A, p, cfmm_type, delta, Gamma, geo_indices):         super(CfmmTradeModel, self).__init__()         self.A = A.to(device)         self.AA = torch.mm(self.A.permute(1, 0), self.A)         self.W = nn.Parameter(torch.rand(A.shape[0], A.shape[0]).to(device))          self.mat1 = nn.Parameter(torch.randn(A.shape[0], A.shape[0]).to(device))                 self.mat2 = nn.Parameter(torch.randn(A.shape[0], A.shape[0]).to(device))         self.bias1 = nn.Parameter(torch.randn(A.shape[0],1).to(device))         self.bias2 = nn.Parameter(torch.randn(A.shape[0],1).to(device))         self.leaky_relu = nn.LeakyReLU(0.1)         self.tanh = nn.Tanh()         self.relu = nn.ReLU()                  self.w = w.to(device)         # self.delta = delta.to(device)         #self.delta = torch.ones(delta.shape).to(device)         #self.delta_scale = nn.Parameter(-3 * torch.ones(1).to(device))          self.delta = nn.Parameter(-3 * torch.ones(delta.shape).to(device))          self.L = None         self.psi_mean = 0         self.cfmm_types = cfmm_types         self.Gamma = Gamma.to(device)         self.p = p.to(device)         self.geo_indices = geo_indices                   self.local_size = self.A.shape[0]         self.global_size = self.A.shape[1]        def device(self) -&gt; str:         return next(self.parameters()).data.device      def psi(self, d, tol=1.0e-1):         ''' Give weight for risk term in utility              Note: Since phi is used in the utility, we must ensure its output                   does not result in a trade transacted with a coin that is not                   available for trade on a given CFMM. This is accounted for                   by mapping psi from global coordinates to local coordinates,                   and then back to global coordinates via A and its transpose.                   This process \"zeros out\" any token values that are \"invalid.\"         '''         sigmoid = nn.Sigmoid()         risk_weight = torch.mm(self.mat1, d) + self.bias1         risk_weight = self.tanh(torch.mm(self.mat2, risk_weight) + self.bias2)          return sigmoid(risk_weight).to(device)      def zero_weights(self):                n_tokens = int(self.A.shape[1] / len(self.cfmm_types))         W_zero = torch.zeros(self.W.data.shape).to(device)                    idx_lo = 0         for j, n_tokens_loc in enumerate(self.geo_indices):             idx_hi = idx_lo + len(n_tokens_loc)             W_zero[idx_lo:idx_hi, idx_lo:idx_hi] = self.W.data[idx_lo:idx_hi,                                                                idx_lo:idx_hi]             idx_lo = idx_hi                          self.W.data = W_zero       def forward(self, d, fxd_pt_tol=1.0e-3, max_iters=int(2.0e3),                  verbose=False, return_obj=False, analytic=False):         ''' Apply Davis-Yin Splitting to minimize f + g + h              Convergence is measured by fixed point residual.              XXX - Force L to be actual Lipschitz constant              Note: Assumes input d = [batch_size, reserves_size]         '''         assert torch.is_tensor(d)           self.zero_weights()          # local coordinate sizes         v = torch.zeros(self.local_size, d.shape[1]).to(device)         z = torch.zeros(self.local_size, d.shape[1]).to(device)          # global coordinate sizes         x = torch.zeros(self.global_size, d.shape[1]).to(device)           y = torch.zeros(self.global_size, d.shape[1]).to(device)                   zeta = torch.cat((v, x, y, z), dim=0)          # delta = torch.exp(self.delta_scale * self.delta)         delta = torch.exp(self.delta)          def prox_f_m(xi, alpha=None):              return prox_f_model(xi, d, self.w, delta, self.geo_indices,                                 self.cfmm_types)          def prox_g_m(xi, alpha=None):             return prox_g_model(xi, d, self.w, delta, self.geo_indices,                                 self.cfmm_types, self.A,                                 self.Gamma)             # psi = self.psi(d) if not analytic else 0.0 * self.psi(d)         # self.psi_mean = torch.mean(psi).detach().to('cpu').numpy()                  psi = torch.ones(1).to(device) if not analytic else torch.zeros(1).to(device)           W = torch.abs(self.W)         # W = torch.zeros(self.W.shape).to(device)         # if not analytic:         #     W[self.W != 0] = torch.abs(self.W[self.W != 0])                  # W = torch.exp(self.W) if not analytic else torch.zeros(self.W.shape).to(device)               def grad_h_m(xi):               return grad_h(xi, self.p, W, self.A,                            psi,                           len(self.cfmm_types))                    WA = torch.mm(W, self.A).to(device)         AWWA = torch.mm(WA.permute(1,0), WA).to(device)          # self.L = 1.0e-5 + torch.linalg.matrix_norm(AWWA, ord=2)                  # self.psi_scaling =  if not analytic else 1.0         self.L = 1.0e-1 + torch.norm(AWWA) * torch.max(psi)         alpha = 1.99 / self.L            with torch.no_grad():              # Bound singular values to unity and zero out off block diagonals             # self.zero_weights()             # u, s, v = torch.svd(self.W.data)             # s = s / s[0]             # #s[s &gt; 1.0] = torch.ones(s[s &gt; 1].shape).to(device)             # self.W.data = torch.mm(torch.mm(u, torch.diag(s)), v.t())              #alpha = min(1.0e2, 0.99 / (float(torch.max(psi_val).cpu() * self.L)))             # alpha = min(1.0e2, 0.99 / self.L)              # print('alpha = ', alpha)                   xi, zeta = apply_DYS(prox_f_m, prox_g_m, grad_h_m, zeta, alpha,                                  fxd_pt_tol=fxd_pt_tol, max_iters=max_iters,                                 verbose=verbose)          # Attach gradients for final iterations         xi, _ = apply_DYS(prox_f_m, prox_g_m, grad_h_m, zeta, alpha,                            fxd_pt_tol=fxd_pt_tol, max_iters=1,                           verbose=verbose)                 q = A.shape[0]         m = A.shape[1]          x_idx = range(q, q+m)         y_idx = range(q+m,q+2*m)                 x = xi[x_idx, :]         y = xi[y_idx, :]                  # Post conditions         tender_nonnegative  = (x &gt;= 0).to('cpu').numpy().all()         receive_nonnegative = (y &gt;= 0).to('cpu').numpy().all()         assert tender_nonnegative         assert receive_nonnegative          check_violation(x, y, d, self.w, self.A, self.Gamma, geo_indices,                         self.cfmm_types, verbose=verbose,                         tol_err=1.0e-1)          return x.permute(1,0), y.permute(1,0) In\u00a0[\u00a0]: Copied! <pre>def check_violation(x, y, d, w, A, Gamma, geo_indices, cfmm_type, \n                    tol_err=1.0e-2, verbose=False):\n\n    assert torch.is_tensor(x)\n    assert torch.is_tensor(y)\n    assert torch.is_tensor(d)\n    assert torch.is_tensor(w)\n    assert torch.is_tensor(A)\n    assert torch.is_tensor(Gamma)        \n    violation = []\n\n    num_samples = x.shape[1] \n    for s in range(num_samples):\n        for i, j in enumerate(geo_indices):\n            geometric = cfmm_type[i]\n            G = Gamma[j[0]:(1+j[-1]), j[0]:(1+j[-1])]\n            if geometric:\n                v = torch.log(d[j, :] +  torch.mm(G, torch.mm(A[j, :], x)) - torch.mm(A[j, :], y))\n                new_val = torch.mm(w[j, :].permute(1,0), v).to('cpu').detach().numpy()[0][s] \n                ref_val = torch.mm(w[j, :].permute(1,0), torch.log(d[j, :])).to('cpu').detach().numpy()[0][s] \n            else:\n                v = d[j] + torch.mm(G, torch.mm(A[j, :], x)) - torch.mm(A[j, :], y)\n                new_val = torch.mm(w[j, :].permute(1,0), v).to('cpu').detach().numpy()[0][s]\n                ref_val = torch.mm(w[j, :].permute(1,0), d[j]).to('cpu').detach().numpy()[0][s]        \n            \n            err = ref_val - new_val\n            if verbose:\n                msg = 'Rel error in phi[{:d}](trade) = {:0.4f}%'\n                print(msg.format(i, 100.0 * err / ref_val))\n            assert err &lt;= tol_err * ref_val\n</pre> def check_violation(x, y, d, w, A, Gamma, geo_indices, cfmm_type,                      tol_err=1.0e-2, verbose=False):      assert torch.is_tensor(x)     assert torch.is_tensor(y)     assert torch.is_tensor(d)     assert torch.is_tensor(w)     assert torch.is_tensor(A)     assert torch.is_tensor(Gamma)             violation = []      num_samples = x.shape[1]      for s in range(num_samples):         for i, j in enumerate(geo_indices):             geometric = cfmm_type[i]             G = Gamma[j[0]:(1+j[-1]), j[0]:(1+j[-1])]             if geometric:                 v = torch.log(d[j, :] +  torch.mm(G, torch.mm(A[j, :], x)) - torch.mm(A[j, :], y))                 new_val = torch.mm(w[j, :].permute(1,0), v).to('cpu').detach().numpy()[0][s]                  ref_val = torch.mm(w[j, :].permute(1,0), torch.log(d[j, :])).to('cpu').detach().numpy()[0][s]              else:                 v = d[j] + torch.mm(G, torch.mm(A[j, :], x)) - torch.mm(A[j, :], y)                 new_val = torch.mm(w[j, :].permute(1,0), v).to('cpu').detach().numpy()[0][s]                 ref_val = torch.mm(w[j, :].permute(1,0), d[j]).to('cpu').detach().numpy()[0][s]                                  err = ref_val - new_val             if verbose:                 msg = 'Rel error in phi[{:d}](trade) = {:0.4f}%'                 print(msg.format(i, 100.0 * err / ref_val))             assert err &lt;= tol_err * ref_val    <p>(First we include a snippet of code for printing trades)</p> In\u00a0[\u00a0]: Copied! <pre>#@title\n\ndef print_trade(x, y):\n    for j, _ in enumerate(x):\n        print('Trades with CFMM', j)\n        for i in range(x[j].shape[0]): \n            val = float(x[j][i].numpy()) \n            print('Tendered {:.2f} of token {:d}.'.format(val, i))\n        for i in range(y[j].shape[0]):\n            val = float(y[j][i].numpy()) \n            print('Received {:.2f} of token {:d}.'.format(val, i))\n</pre> #@title  def print_trade(x, y):     for j, _ in enumerate(x):         print('Trades with CFMM', j)         for i in range(x[j].shape[0]):              val = float(x[j][i].numpy())              print('Tendered {:.2f} of token {:d}.'.format(val, i))         for i in range(y[j].shape[0]):             val = float(y[j][i].numpy())              print('Received {:.2f} of token {:d}.'.format(val, i)) In\u00a0[\u00a0]: Copied! <pre># Tolerances for each CFMM\ndeltas = 1.0e-2 * torch.ones(5,1).to(device) #[torch.zeros(1).to(device) for delta in range(5)]\ncfmm_sizes = [3, 2, 2, 2, 2]\ngeo_indices = get_cfmm_indices(cfmm_sizes)\n\n# Trade fee parameters in each CFMM\ngammas = [0.98, 0.99, 0.98, 0.99, 0.98]\nGamma = [gamma * torch.eye(cfmm_sizes[j]).to(device)\n         for j, gamma in enumerate(gammas)]\nGamma = get_block_diag(Gamma).to(device)\n\n# Weights for the modified means in each CFMM\nw1 = torch.tensor([[0.2], [0.3], [0.5]]).to(device)\nw2 = torch.tensor([[0.4], [0.6]]).to(device)\nw3 = torch.tensor([[0.5], [0.5]]).to(device)\nw4 = torch.tensor([[0.6], [0.4]]).to(device)\nw5 = torch.tensor([[0.6], [0.4]]).to(device)\nw = torch.cat((w1, w2, w3, w4, w5), dim=0).to(device)\n\n# Label for which CFMMs uses geometric means (otherwise use arithmetic)\ncfmm_types = [True, True, True, True, False]\ncfmm_types = [float(cfmm_type) * torch.ones(1).to(device) for cfmm_type in cfmm_types] \n\n# Transformations from global to local coordinates of each CFMM\nA1 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]).to(device)\nA2 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]).to(device)\nA3 = torch.tensor([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]).to(device)\nA4 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]).to(device)\nA5 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]).to(device)\nA = [A1, A2, A3, A4, A5]\nA = get_block_diag(A).to(device)\nprint('A = ', A)\nprint('A.shape = ', A.shape)\n\n# Define intrinsic value of assets\np = torch.tensor([[1.0], [1.0], [1.0]]).to(device)\n\n# Trade model for CFMM network\nthe_model = CfmmTradeModel(w, A, p, cfmm_types, deltas, Gamma, geo_indices)\nprint('model loaded successfully')\n\n# Sample data for reserves in CFMM network\nd1 = torch.tensor([[5.0], [7.3], [12.5]]).to(device)\nd2 = torch.tensor([[10.0], [4.7]]).to(device)\nd3 = torch.tensor([[5.5], [7.4]]).to(device)\nd4 = torch.tensor([[4.6], [14.9]]).to(device)\nd5 = torch.tensor([[7.6], [5.4]]).to(device)\nd = torch.cat((d1, d2, d3, d4, d5), dim=0).to(device)\n#d = d.permute(1,0).clone()\n\n# Optimal trade for provided data\nx, y = the_model(d, verbose=True)\n\nprint('x = ', x)\nprint('y = ', y)\n</pre> # Tolerances for each CFMM deltas = 1.0e-2 * torch.ones(5,1).to(device) #[torch.zeros(1).to(device) for delta in range(5)] cfmm_sizes = [3, 2, 2, 2, 2] geo_indices = get_cfmm_indices(cfmm_sizes)  # Trade fee parameters in each CFMM gammas = [0.98, 0.99, 0.98, 0.99, 0.98] Gamma = [gamma * torch.eye(cfmm_sizes[j]).to(device)          for j, gamma in enumerate(gammas)] Gamma = get_block_diag(Gamma).to(device)  # Weights for the modified means in each CFMM w1 = torch.tensor([[0.2], [0.3], [0.5]]).to(device) w2 = torch.tensor([[0.4], [0.6]]).to(device) w3 = torch.tensor([[0.5], [0.5]]).to(device) w4 = torch.tensor([[0.6], [0.4]]).to(device) w5 = torch.tensor([[0.6], [0.4]]).to(device) w = torch.cat((w1, w2, w3, w4, w5), dim=0).to(device)  # Label for which CFMMs uses geometric means (otherwise use arithmetic) cfmm_types = [True, True, True, True, False] cfmm_types = [float(cfmm_type) * torch.ones(1).to(device) for cfmm_type in cfmm_types]   # Transformations from global to local coordinates of each CFMM A1 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]).to(device) A2 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]).to(device) A3 = torch.tensor([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]).to(device) A4 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]).to(device) A5 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]).to(device) A = [A1, A2, A3, A4, A5] A = get_block_diag(A).to(device) print('A = ', A) print('A.shape = ', A.shape)  # Define intrinsic value of assets p = torch.tensor([[1.0], [1.0], [1.0]]).to(device)  # Trade model for CFMM network the_model = CfmmTradeModel(w, A, p, cfmm_types, deltas, Gamma, geo_indices) print('model loaded successfully')  # Sample data for reserves in CFMM network d1 = torch.tensor([[5.0], [7.3], [12.5]]).to(device) d2 = torch.tensor([[10.0], [4.7]]).to(device) d3 = torch.tensor([[5.5], [7.4]]).to(device) d4 = torch.tensor([[4.6], [14.9]]).to(device) d5 = torch.tensor([[7.6], [5.4]]).to(device) d = torch.cat((d1, d2, d3, d4, d5), dim=0).to(device) #d = d.permute(1,0).clone()  # Optimal trade for provided data x, y = the_model(d, verbose=True)  print('x = ', x) print('y = ', y) In\u00a0[\u00a0]: Copied! <pre>import json\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data.dataset import random_split\n\nrel_token_vals_ave = torch.tensor([[1.0], [1.9], [0.5]])\nres_ave = [50.0, 30.0, 23.5, 35.7, 46.9]\nthe_model.p = rel_token_vals_ave\n\n\ndef get_token_val_sample(rel_vals, var):\n    sample = torch.tensor([[(1.0 + var * torch.randn(1)) * val.numpy()]\n              for _, val in enumerate(rel_vals)])\n    return sample\n\ndef create_loader(A, n_tokens, cfmm_indices, train_size=5000, test_size=500,\n                  train_batch_size=100, test_batch_size=500):\n\n    var_per = 1.0e-1\n   \n    torch.manual_seed(2022)\n    d = []\n    \n    batch_size = train_size + test_size\n\n    for j, idx in enumerate(cfmm_indices): \n        \n        A_block = A[idx, j*n_tokens:(j+1)*n_tokens]\n\n        loc_rel_token_vals = torch.mm(A_block.to('cpu'), rel_token_vals_ave)\n\n        rel_token_vals_sample = loc_rel_token_vals\n        for k in range(batch_size):\n            new_sample = get_token_val_sample(loc_rel_token_vals, var_per)\n            if k &gt; 0:\n                rel_token_vals_sample = torch.cat((rel_token_vals_sample, new_sample), dim=1)\n            else:\n                rel_token_vals_sample = new_sample \n\n        num_tokens = len(idx)\n        reserves = torch.zeros(num_tokens, batch_size)\n        for t in range(num_tokens):\n            res_samples = (1.0 + var_per * torch.randn(batch_size)) * res_ave[j]\n\n            reserves[t, :] = float(w[idx, :][t]) * rel_token_vals_sample[0, :]\n            reserves[t, :] *= res_samples \n            reserves[t, :] /= (float(w[idx, :][0]) * rel_token_vals_sample[t, :])\n\n        reserves = torch.tensor(reserves)\n        d.append(reserves)\n\n    d_ten = d[0] \n    for idx, _ in enumerate(d):\n        if idx &gt; 0:\n            d_ten = torch.cat((d_ten, d[idx]), dim=0) \n\n    d_ten = d_ten.permute(1,0)  \n\n    # --------------------------------------------------------------------------\n    # Analytic predictions with no noise\n    # --------------------------------------------------------------------------\n    # anal_model = CfmmTradeModel(w, A, p, cfmm_types, deltas, Gamma, geo_indices) \n    # anal_model.eval()\n    # with torch.no_grad():\n    #     for j, val in enumerate(anal_model.W): \n    #         anal_model.W[j][0]= torch.zeros(val[0].shape)           \n    \n    # x_true, y_true = anal_model(d_ten.permute(1,0).to(device),\n    #                             max_iters=int(1.0e4))\n    # x_true = x_true.detach()\n    # y_true = y_true.detach()\n    # --------------------------------------------------------------------------\n    # --------------------------------------------------------------------------\n    std = 1.0e-3 * torch.rand(d_ten.shape[0], 1)\n    std = std * torch.ones(d_ten.shape)  \n    noise = torch.normal(mean=torch.zeros(d_ten.shape), std=std) \n    d_obs = (1.0 + noise) * d_ten \n    \n    dataset = TensorDataset(d_obs.to(device), d_ten.to(device))\n    # dataset = TensorDataset(d_obs.to(device), d_ten.to(device), x_true.to(device), y_true.to(device))\n\n    train_data, test_data = random_split(dataset, [train_size,\n                                                   test_size])\n    loader_train = DataLoader(dataset=train_data,\n                              batch_size=train_batch_size, shuffle=True)\n    loader_test = DataLoader(dataset=test_data,\n                             batch_size=test_batch_size, shuffle=False)\n\n    return loader_train, loader_test\n\ndef convert_to_tensor(x_list):\n    x_ten = x_list[0]\n    x_ten = x_list[0]\n    for idx, val in enumerate(x_list):\n        if idx &gt; 0:\n            x_ten = torch.cat((x_ten, val), dim=0)\n    x_ten = x_ten.permute(1,0)  \n    return x_ten\n</pre> import json from torch.utils.data import TensorDataset, DataLoader from torch.utils.data.dataset import random_split  rel_token_vals_ave = torch.tensor([[1.0], [1.9], [0.5]]) res_ave = [50.0, 30.0, 23.5, 35.7, 46.9] the_model.p = rel_token_vals_ave   def get_token_val_sample(rel_vals, var):     sample = torch.tensor([[(1.0 + var * torch.randn(1)) * val.numpy()]               for _, val in enumerate(rel_vals)])     return sample  def create_loader(A, n_tokens, cfmm_indices, train_size=5000, test_size=500,                   train_batch_size=100, test_batch_size=500):      var_per = 1.0e-1         torch.manual_seed(2022)     d = []          batch_size = train_size + test_size      for j, idx in enumerate(cfmm_indices):                   A_block = A[idx, j*n_tokens:(j+1)*n_tokens]          loc_rel_token_vals = torch.mm(A_block.to('cpu'), rel_token_vals_ave)          rel_token_vals_sample = loc_rel_token_vals         for k in range(batch_size):             new_sample = get_token_val_sample(loc_rel_token_vals, var_per)             if k &gt; 0:                 rel_token_vals_sample = torch.cat((rel_token_vals_sample, new_sample), dim=1)             else:                 rel_token_vals_sample = new_sample           num_tokens = len(idx)         reserves = torch.zeros(num_tokens, batch_size)         for t in range(num_tokens):             res_samples = (1.0 + var_per * torch.randn(batch_size)) * res_ave[j]              reserves[t, :] = float(w[idx, :][t]) * rel_token_vals_sample[0, :]             reserves[t, :] *= res_samples              reserves[t, :] /= (float(w[idx, :][0]) * rel_token_vals_sample[t, :])          reserves = torch.tensor(reserves)         d.append(reserves)      d_ten = d[0]      for idx, _ in enumerate(d):         if idx &gt; 0:             d_ten = torch.cat((d_ten, d[idx]), dim=0)       d_ten = d_ten.permute(1,0)        # --------------------------------------------------------------------------     # Analytic predictions with no noise     # --------------------------------------------------------------------------     # anal_model = CfmmTradeModel(w, A, p, cfmm_types, deltas, Gamma, geo_indices)      # anal_model.eval()     # with torch.no_grad():     #     for j, val in enumerate(anal_model.W):      #         anal_model.W[j][0]= torch.zeros(val[0].shape)                     # x_true, y_true = anal_model(d_ten.permute(1,0).to(device),     #                             max_iters=int(1.0e4))     # x_true = x_true.detach()     # y_true = y_true.detach()     # --------------------------------------------------------------------------     # --------------------------------------------------------------------------     std = 1.0e-3 * torch.rand(d_ten.shape[0], 1)     std = std * torch.ones(d_ten.shape)       noise = torch.normal(mean=torch.zeros(d_ten.shape), std=std)      d_obs = (1.0 + noise) * d_ten           dataset = TensorDataset(d_obs.to(device), d_ten.to(device))     # dataset = TensorDataset(d_obs.to(device), d_ten.to(device), x_true.to(device), y_true.to(device))      train_data, test_data = random_split(dataset, [train_size,                                                    test_size])     loader_train = DataLoader(dataset=train_data,                               batch_size=train_batch_size, shuffle=True)     loader_test = DataLoader(dataset=test_data,                              batch_size=test_batch_size, shuffle=False)      return loader_train, loader_test  def convert_to_tensor(x_list):     x_ten = x_list[0]     x_ten = x_list[0]     for idx, val in enumerate(x_list):         if idx &gt; 0:             x_ten = torch.cat((x_ten, val), dim=0)     x_ten = x_ten.permute(1,0)       return x_ten   In\u00a0[\u00a0]: Copied! <pre>## REPLACE THIS WITH BISECTION METHOD\n\ndef correct_trade(x, y, d, p, A, Gamma, ww, geo_indices, cfmm_type, n_tokens, newton_iters=200):\n    y_correct = []\n\n    y_correct = torch.zeros(y.shape).to(device)\n\n    for j, idx in enumerate(geo_indices): \n        tau = 0.0\n        #iter = 0\n        #invalid_trade = True\n\n        #while iter &lt; newton_iters and invalid_trade:\n        #    iter = iter + 1\n        for iter in range(newton_iters): \n            \n            geometric = cfmm_type[j]\n\n            G = Gamma[idx[0]:(1+idx[-1]), idx[0]:(1+idx[-1])] \n            if geometric:\n                 \n                ytp = y[n_tokens*j:(j+1)*n_tokens, :] - tau * p\n                ytp = ytp.to(device)\n                Aytp =  torch.mm(A[idx, n_tokens*j:(j+1)*n_tokens], ytp).to(device)\n                Ax = torch.mm(A[idx, :], x).to(device)\n                GAxAytp =  (torch.mm(G, Ax) - Aytp).to(device) \n                d_block = d[idx, :].to(device)\n\n                v = torch.log(d_block + GAxAytp).to(device)   \n                v = torch.clamp(v, min=1.0e-6).to(device) \n                \n                phi_z = torch.mm(ww[idx, :].permute(1,0), v)[0]\n                phi_d = torch.mm(ww[idx, :].permute(1,0), torch.log(d[idx, :]))[0]\n\n                theta_z = phi_z - phi_d\n\n                # invalid_trade = (theta_z &lt; 0).any()\n\n                dot_product = torch.mm(torch.mm(p.permute(1,0), A[idx, n_tokens*j:(j+1)*n_tokens].permute(1,0)), ww[idx, :] / v)\n\n                if iter &lt; 20:\n                    tau =  tau - (0.05 / float(iter + 1)) * theta_z\n                else:\n                    tau = tau - theta_z / (phi_z * dot_product)\n            else:\n                v = d[idx, :] +  torch.mm(G, torch.mm(A[idx, :], x)) - torch.mm(A[idx, n_tokens*j:(j+1)*n_tokens], y[n_tokens*j:(j+1)*n_tokens, :] - tau * p)\n                theta_z = torch.mm(ww[idx, :].permute(1,0), v - d[idx, :])\n\n                if iter &lt; 20:\n                    tau = tau - (0.05 / float(iter + 1)) * theta_z \n                else:\n                    tau = tau - theta_z / torch.mm(ww[idx, :].permute(1,0), torch.mm(A[idx, n_tokens*j:(j+1)*n_tokens], p))                \n\n        y_block =  y[n_tokens*j:(j+1)*n_tokens, :] - tau * p\n        y_correct[n_tokens*j:(j+1)*n_tokens, :] = y_block\n\n    check_violation(x, y_correct, d, ww, A, Gamma, geo_indices, cfmm_types)\n    return x, y_correct\n</pre>  ## REPLACE THIS WITH BISECTION METHOD  def correct_trade(x, y, d, p, A, Gamma, ww, geo_indices, cfmm_type, n_tokens, newton_iters=200):     y_correct = []      y_correct = torch.zeros(y.shape).to(device)      for j, idx in enumerate(geo_indices):          tau = 0.0         #iter = 0         #invalid_trade = True          #while iter &lt; newton_iters and invalid_trade:         #    iter = iter + 1         for iter in range(newton_iters):                           geometric = cfmm_type[j]              G = Gamma[idx[0]:(1+idx[-1]), idx[0]:(1+idx[-1])]              if geometric:                                   ytp = y[n_tokens*j:(j+1)*n_tokens, :] - tau * p                 ytp = ytp.to(device)                 Aytp =  torch.mm(A[idx, n_tokens*j:(j+1)*n_tokens], ytp).to(device)                 Ax = torch.mm(A[idx, :], x).to(device)                 GAxAytp =  (torch.mm(G, Ax) - Aytp).to(device)                  d_block = d[idx, :].to(device)                  v = torch.log(d_block + GAxAytp).to(device)                    v = torch.clamp(v, min=1.0e-6).to(device)                                   phi_z = torch.mm(ww[idx, :].permute(1,0), v)[0]                 phi_d = torch.mm(ww[idx, :].permute(1,0), torch.log(d[idx, :]))[0]                  theta_z = phi_z - phi_d                  # invalid_trade = (theta_z &lt; 0).any()                  dot_product = torch.mm(torch.mm(p.permute(1,0), A[idx, n_tokens*j:(j+1)*n_tokens].permute(1,0)), ww[idx, :] / v)                  if iter &lt; 20:                     tau =  tau - (0.05 / float(iter + 1)) * theta_z                 else:                     tau = tau - theta_z / (phi_z * dot_product)             else:                 v = d[idx, :] +  torch.mm(G, torch.mm(A[idx, :], x)) - torch.mm(A[idx, n_tokens*j:(j+1)*n_tokens], y[n_tokens*j:(j+1)*n_tokens, :] - tau * p)                 theta_z = torch.mm(ww[idx, :].permute(1,0), v - d[idx, :])                  if iter &lt; 20:                     tau = tau - (0.05 / float(iter + 1)) * theta_z                  else:                     tau = tau - theta_z / torch.mm(ww[idx, :].permute(1,0), torch.mm(A[idx, n_tokens*j:(j+1)*n_tokens], p))                          y_block =  y[n_tokens*j:(j+1)*n_tokens, :] - tau * p         y_correct[n_tokens*j:(j+1)*n_tokens, :] = y_block      check_violation(x, y_correct, d, ww, A, Gamma, geo_indices, cfmm_types)     return x, y_correct In\u00a0[\u00a0]: Copied! <pre>def compute_energy(d, ww, A, geo_indices, cfmm_type):\n    ''' Get the energy violation of system\n    '''\n    energy = torch.zeros(len(geo_indices), d.shape[1]).to(device)\n\n    for j, idx in enumerate(geo_indices): \n        geometric = cfmm_type[j]\n\n        if geometric:\n            term = torch.mm(ww[idx, :].permute(1,0), torch.log(d[idx, :]))\n            energy[j, :] += torch.sum(torch.exp(term))\n        else: \n            energy[j, :] += torch.sum(torch.mm(ww[idx, :].permute(1,0), d[idx, :]))\n    return energy\n\n\ndef get_violation(x, y, d, w, A, Gamma, geo_indices, cfmm_type):\n    relu = nn.ReLU()\n    energy_new = compute_energy(d + torch.mm(Gamma, torch.mm(A, x)) - torch.mm(A,y), w, A, geo_indices, cfmm_type)\n    energy_old = compute_energy(d, w, A, geo_indices, cfmm_type) \n    error = relu(energy_old - energy_new)\n\n    error_mean = torch.sum(torch.mean(error, dim=1))\n\n    error_rel = torch.sum(torch.mean(error, dim=1)) / torch.sum(torch.mean(energy_old))\n    return error_mean, error_rel\n</pre> def compute_energy(d, ww, A, geo_indices, cfmm_type):     ''' Get the energy violation of system     '''     energy = torch.zeros(len(geo_indices), d.shape[1]).to(device)      for j, idx in enumerate(geo_indices):          geometric = cfmm_type[j]          if geometric:             term = torch.mm(ww[idx, :].permute(1,0), torch.log(d[idx, :]))             energy[j, :] += torch.sum(torch.exp(term))         else:              energy[j, :] += torch.sum(torch.mm(ww[idx, :].permute(1,0), d[idx, :]))     return energy   def get_violation(x, y, d, w, A, Gamma, geo_indices, cfmm_type):     relu = nn.ReLU()     energy_new = compute_energy(d + torch.mm(Gamma, torch.mm(A, x)) - torch.mm(A,y), w, A, geo_indices, cfmm_type)     energy_old = compute_energy(d, w, A, geo_indices, cfmm_type)      error = relu(energy_old - energy_new)      error_mean = torch.sum(torch.mean(error, dim=1))      error_rel = torch.sum(torch.mean(error, dim=1)) / torch.sum(torch.mean(energy_old))     return error_mean, error_rel In\u00a0[\u00a0]: Copied! <pre>def get_list_from_ten(d, w):\n    d_list = []\n    idx_lo = 0\n    for j, _ in enumerate(w):\n        idx_hi = idx_lo + w[j].shape[0]\n        d_list.append(d[:, idx_lo:idx_hi].permute(1,0))\n        idx_lo = idx_hi\n    return d_list\n\ndef get_ten_from_list(d):\n    d_ten = torch.zeros(d[0].shape)\n\n    for j, val in enumerate(d):\n        d_ten = torch.cat((d_ten, val), dim=0)\n    return d_ten    \n    \ndef utility(x, y, p):\n    n_cfmms = int(x.shape[0] / p.shape[0])\n    obj = torch.zeros(1, x.shape[1]).to(device)\n    for j in range(n_cfmms):\n        yx = (y - x).to(device)\n        obj = obj + torch.mm(p.permute(1,0), yx[j*p.shape[0]:(j+1)*p.shape[0]])\n    return obj\n</pre> def get_list_from_ten(d, w):     d_list = []     idx_lo = 0     for j, _ in enumerate(w):         idx_hi = idx_lo + w[j].shape[0]         d_list.append(d[:, idx_lo:idx_hi].permute(1,0))         idx_lo = idx_hi     return d_list  def get_ten_from_list(d):     d_ten = torch.zeros(d[0].shape)      for j, val in enumerate(d):         d_ten = torch.cat((d_ten, val), dim=0)     return d_ten          def utility(x, y, p):     n_cfmms = int(x.shape[0] / p.shape[0])     obj = torch.zeros(1, x.shape[1]).to(device)     for j in range(n_cfmms):         yx = (y - x).to(device)         obj = obj + torch.mm(p.permute(1,0), yx[j*p.shape[0]:(j+1)*p.shape[0]])     return obj     In\u00a0[\u00a0]: Copied! <pre>from prettytable import PrettyTable\n\nn_tokens = 3\nloader_train, loader_test = create_loader(A, n_tokens, geo_indices)\nnum_epochs    = int(1.0e3)\nlearning_rate = 5.0e-3\noptimizer     = torch.optim.Adam(the_model.parameters(), lr=learning_rate)\nlr_scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5,\n                                                gamma=0.98)\ntraining_msg  = '[{:4d}] Contraint Error = {:6.2f}%'\ntraining_msg += '| loss = {:8.1e} | pred util = {:8.1e} | fro-norm = {:2.2e} '\ntraining_msg += '| psi = {:2.3e} | lr = {:2.2e}'\n\nMSE = nn.MSELoss()\n\ndef model_params(net):\n    table = PrettyTable([\"Network Component\", \"# Parameters\"])\n    num_params = 0\n    for name, parameter in net.named_parameters():\n        if not parameter.requires_grad:\n            continue\n        table.add_row([name, parameter.numel()])\n        num_params += parameter.numel()\n    table.add_row(['TOTAL', num_params])\n    return table\n\nprint(model_params(the_model))\n</pre> from prettytable import PrettyTable  n_tokens = 3 loader_train, loader_test = create_loader(A, n_tokens, geo_indices) num_epochs    = int(1.0e3) learning_rate = 5.0e-3 optimizer     = torch.optim.Adam(the_model.parameters(), lr=learning_rate) lr_scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5,                                                 gamma=0.98) training_msg  = '[{:4d}] Contraint Error = {:6.2f}%' training_msg += '| loss = {:8.1e} | pred util = {:8.1e} | fro-norm = {:2.2e} ' training_msg += '| psi = {:2.3e} | lr = {:2.2e}'  MSE = nn.MSELoss()  def model_params(net):     table = PrettyTable([\"Network Component\", \"# Parameters\"])     num_params = 0     for name, parameter in net.named_parameters():         if not parameter.requires_grad:             continue         table.add_row([name, parameter.numel()])         num_params += parameter.numel()     table.add_row(['TOTAL', num_params])     return table  print(model_params(the_model)) In\u00a0[\u00a0]: Copied! <pre>loss_ave = 0.0\nobj_ave = 0.0\n \nthe_model.to(device)\n\nfor epoch in range(num_epochs): \n    for d_obs, d_true in loader_train:\n        optimizer.zero_grad()\n        the_model.train() \n        x, y = the_model(d_obs.permute(1,0).to(device), max_iters=int(1.0e3))\n        \n        obj = torch.mean(utility(x.permute(1,0), y.permute(1,0), the_model.p.to(device)))\n        viol, rel_viol = get_violation(x.permute(1,0), y.permute(1,0),\n                                       d_true.permute(1,0), w, A, Gamma,\n                                       geo_indices, cfmm_types)\n        \n        loss = 1.0e-1 * (viol ** 2) - obj\n\n        loss.backward()\n        optimizer.step()\n\n        loss_curr = loss.detach().item()\n        loss_ave *= 0.9\n        loss_ave += 0.1 * loss_curr if loss_ave != 0 else loss_curr     \n\n        W = torch.abs(the_model.W.data.clone()) \n\n        print(training_msg.format(epoch, 100 * rel_viol, loss_ave, obj,\n                                  torch.norm(W),\n                                  the_model.psi_mean,\n                                  optimizer.param_groups[0]['lr']\n                                  ))\n        print('delta = ', the_model.delta)\n    lr_scheduler.step()\n</pre> loss_ave = 0.0 obj_ave = 0.0   the_model.to(device)  for epoch in range(num_epochs):      for d_obs, d_true in loader_train:         optimizer.zero_grad()         the_model.train()          x, y = the_model(d_obs.permute(1,0).to(device), max_iters=int(1.0e3))                  obj = torch.mean(utility(x.permute(1,0), y.permute(1,0), the_model.p.to(device)))         viol, rel_viol = get_violation(x.permute(1,0), y.permute(1,0),                                        d_true.permute(1,0), w, A, Gamma,                                        geo_indices, cfmm_types)                  loss = 1.0e-1 * (viol ** 2) - obj          loss.backward()         optimizer.step()          loss_curr = loss.detach().item()         loss_ave *= 0.9         loss_ave += 0.1 * loss_curr if loss_ave != 0 else loss_curr               W = torch.abs(the_model.W.data.clone())           print(training_msg.format(epoch, 100 * rel_viol, loss_ave, obj,                                   torch.norm(W),                                   the_model.psi_mean,                                   optimizer.param_groups[0]['lr']                                   ))         print('delta = ', the_model.delta)     lr_scheduler.step() In\u00a0[\u00a0]: Copied! <pre>print(the_model.delta)\nprint(the_model.delta_scale)\n</pre> print(the_model.delta) print(the_model.delta_scale) <p>Print of heatmap of matrix</p> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nW = the_model.W.data.clone()\nW[W != 0] = torch.exp(W[W != 0])\n\nplt.imshow(W.to('cpu').detach().numpy(), interpolation='nearest')\nplt.colorbar()\nplt.show()\n\n# CONFUSION MATRIX?\n\nprint(the_model.W.data.shape)\n</pre> from matplotlib import pyplot as plt W = the_model.W.data.clone() W[W != 0] = torch.exp(W[W != 0])  plt.imshow(W.to('cpu').detach().numpy(), interpolation='nearest') plt.colorbar() plt.show()  # CONFUSION MATRIX?  print(the_model.W.data.shape) <p>Load analytic model and set weights $W$ to be zero, i.e. no correction term.</p> In\u00a0[\u00a0]: Copied! <pre># anal_model = CfmmTradeModel(w, A, p, cfmm_types, deltas, Gamma, geo_indices)\n# with torch.no_grad():\n#     for j, val in enumerate(anal_model.W): \n#         anal_model.W[j][0]= torch.zeros(val[0].shape)\n</pre> # anal_model = CfmmTradeModel(w, A, p, cfmm_types, deltas, Gamma, geo_indices) # with torch.no_grad(): #     for j, val in enumerate(anal_model.W):  #         anal_model.W[j][0]= torch.zeros(val[0].shape)        In\u00a0[\u00a0]: Copied! <pre># XXX - Loss Testing stuff....\n\n#for d_obs, d_true, x_true, y_true in loader_test:                        \nfor d_obs, d_true in loader_test:   \n    the_model.eval() \n    x_l2o, y_l2o = the_model(d_obs.permute(1,0).to(device), max_iters=int(1e3))\n\n    x_anal, y_anal = the_model(d_obs.permute(1,0).to(device), max_iters=int(2e3),\n                               analytic=True)\n\n    # x_anal, y_anal = anal_model(d_obs.permute(1,0).to(device), max_iters=int(1e3))\n\n    # d_true = get_list_from_ten(d_true, w)\n\n    # x_l2o_corr, y_l2o_corr = correct_trade(x_l2o.permute(1,0),\n    #                                          y_l2o.permute(1,0), \n    #                                          d_true.permute(1,0), \n    #                                          the_model.p.to(device), A, Gamma, \n    #                                          w, geo_indices, cfmm_types,\n    #                                          n_tokens)\n    \n    # x_anal_corr, y_anal_corr = correct_trade(x_anal.permute(1,0),\n    #                                          y_anal.permute(1,0), \n    #                                          d_true.permute(1,0), \n    #                                          the_model.p.to(device), A, Gamma, \n    #                                          w, geo_indices, cfmm_types,\n    #                                          n_tokens)\n    \n    #y_ten_l2o = get_ten_from_list(y_l2o)\n    #y_corr_l2o = get_ten_from_list(y_corr_l2o)\n#mse = MSE(y_l2o, y_true) + MSE(x_l2o, x_true)\n#print('y_l2o = ', y_l2o)\n#print('y_true = ', y_true)\n#print('mse = ', mse)\n</pre> # XXX - Loss Testing stuff....  #for d_obs, d_true, x_true, y_true in loader_test:                         for d_obs, d_true in loader_test:        the_model.eval()      x_l2o, y_l2o = the_model(d_obs.permute(1,0).to(device), max_iters=int(1e3))      x_anal, y_anal = the_model(d_obs.permute(1,0).to(device), max_iters=int(2e3),                                analytic=True)      # x_anal, y_anal = anal_model(d_obs.permute(1,0).to(device), max_iters=int(1e3))      # d_true = get_list_from_ten(d_true, w)      # x_l2o_corr, y_l2o_corr = correct_trade(x_l2o.permute(1,0),     #                                          y_l2o.permute(1,0),      #                                          d_true.permute(1,0),      #                                          the_model.p.to(device), A, Gamma,      #                                          w, geo_indices, cfmm_types,     #                                          n_tokens)          # x_anal_corr, y_anal_corr = correct_trade(x_anal.permute(1,0),     #                                          y_anal.permute(1,0),      #                                          d_true.permute(1,0),      #                                          the_model.p.to(device), A, Gamma,      #                                          w, geo_indices, cfmm_types,     #                                          n_tokens)          #y_ten_l2o = get_ten_from_list(y_l2o)     #y_corr_l2o = get_ten_from_list(y_corr_l2o) #mse = MSE(y_l2o, y_true) + MSE(x_l2o, x_true) #print('y_l2o = ', y_l2o) #print('y_true = ', y_true) #print('mse = ', mse) In\u00a0[\u00a0]: Copied! <pre>def check_constraints(x, y, d, w, A, Gamma, geo_indices, cfmm_type):\n    relu = nn.ReLU()\n    energy_new = compute_energy(d + torch.mm(Gamma, torch.mm(A, x)) - torch.mm(A,y), w, A, geo_indices, cfmm_type)\n    energy_old = compute_energy(d, w, A, geo_indices, cfmm_type) \n    constraint_viol = relu(energy_old - energy_new)\n    return constraint_viol\n</pre> def check_constraints(x, y, d, w, A, Gamma, geo_indices, cfmm_type):     relu = nn.ReLU()     energy_new = compute_energy(d + torch.mm(Gamma, torch.mm(A, x)) - torch.mm(A,y), w, A, geo_indices, cfmm_type)     energy_old = compute_energy(d, w, A, geo_indices, cfmm_type)      constraint_viol = relu(energy_old - energy_new)     return constraint_viol In\u00a0[\u00a0]: Copied! <pre>util_l2o = utility(x_l2o.to(device).permute(1,0), y_l2o.to(device).permute(1,0), the_model.p.to(device))\n\nprint('util_l2o = ', util_l2o)\nconstraint_viol = check_constraints(x_l2o.permute(1,0), y_l2o.permute(1,0),\n                                    d_true.permute(1,0), w, A, Gamma,\n                                    geo_indices, cfmm_types)\n\nconstraint_viol = torch.sum(constraint_viol, dim=0, keepdim=True)\nutil_l2o[constraint_viol &gt; 0] = 0.0 * util_l2o[constraint_viol &gt; 0] \n\nprint('util_l2o = ', torch.mean(util_l2o))\n\n\nutil_anal = utility(x_anal.permute(1,0), y_anal.permute(1,0), the_model.p.to(device))\nconstraint_viol = check_constraints(x_anal.permute(1,0), y_anal.permute(1,0),\n                                    d_true.permute(1,0), w, A, Gamma,\n                                    geo_indices, cfmm_types)\n\nconstraint_viol = torch.sum(constraint_viol, dim=0, keepdim=True)\nprint('pre_anal = ', util_anal)\n\nutil_anal[constraint_viol &gt; 0] = 0.0 * util_anal[constraint_viol &gt; 0] \n\n\nprint('util_anal = ', torch.mean(util_anal))\n</pre> util_l2o = utility(x_l2o.to(device).permute(1,0), y_l2o.to(device).permute(1,0), the_model.p.to(device))  print('util_l2o = ', util_l2o) constraint_viol = check_constraints(x_l2o.permute(1,0), y_l2o.permute(1,0),                                     d_true.permute(1,0), w, A, Gamma,                                     geo_indices, cfmm_types)  constraint_viol = torch.sum(constraint_viol, dim=0, keepdim=True) util_l2o[constraint_viol &gt; 0] = 0.0 * util_l2o[constraint_viol &gt; 0]   print('util_l2o = ', torch.mean(util_l2o))   util_anal = utility(x_anal.permute(1,0), y_anal.permute(1,0), the_model.p.to(device)) constraint_viol = check_constraints(x_anal.permute(1,0), y_anal.permute(1,0),                                     d_true.permute(1,0), w, A, Gamma,                                     geo_indices, cfmm_types)  constraint_viol = torch.sum(constraint_viol, dim=0, keepdim=True) print('pre_anal = ', util_anal)  util_anal[constraint_viol &gt; 0] = 0.0 * util_anal[constraint_viol &gt; 0]    print('util_anal = ', torch.mean(util_anal))"},{"location":"exp-crypto/#cryptoasset-trading","title":"Cryptoasset Trading\u00b6","text":"<p>The problem at hand is to train a model that reliably identifies arbitrage transactions for cryptoasssets within a network of constant function market makers (CFMMs). That is, given private valuation $p_i$ of the $i$-th cryptoasset, an analytic form of the task at hand is expressed via</p> <p>$$ \\min_{(x,y)} - U(x,y)\\ \\ \\text{s.t.} \\ \\ (x^j,y^j) \\ \\ \\text{forms a valid trade within the $j$-th CFMM with reserves $r^j$},$$</p> <p>where $r,p,x,y\\in\\mathbb{R}^n$ and the utility is</p> <p>$$ U(x,y) = \\sum_{j} \\left&lt;  A^jp, A^j(y^j - x^j)\\right&gt;,$$</p> <p>where $A^j$ maps $x^j$ and $y^j$ from global coordinates to local coordinates. (Note local coordinates may be in a smaller dimensional space since not all CFMMs utilize all cryptoassets.)</p> <p>As a simplified model of real-world phenomena, we introduce noise (e.g. due to front running) into the reserves $r$ to obtain noisy observable data $d$. This makes the problem inconsistent (we note below a simple mechanism for correcting this for this toy problem). To account for noise, a \"cost of risk\" is learned and incorporated in a data-driven utility $U_\\Theta$. This utility is optimized for a collection of training data to identify the optimal arbitrage trades. That is, we define an implicit model</p> <p>$$ \\mathcal{N}_\\Theta(d) \\triangleq (x_\\Theta, y_\\Theta) \\triangleq \\text{argmin}_{(x,y)} -U_\\Theta(x,y) \\ \\ \\text{s.t.} \\ \\ (x^j,y^j) \\ \\ \\text{forms a valid trade within the $j$-th CFMM with r}.$$</p> <p>The weights $\\Theta$ are tuned to solve the training problem</p> <p>$$ \\min_{\\Theta} \\mathbb{E}_{d\\sim\\mathcal{D}} \\left[  - U_\\Theta(x_\\Theta, \\tilde{y}_\\Theta)  \\right],$$</p> <p>where</p> <p>$$ \\tilde{y}_\\Theta^j = y_\\Theta^j - \\tau_j p$$</p> <p>and $\\tau_j$ is chosen so $(x^j_\\Theta, \\tilde{y}^j_\\Theta)$ forms a valid trade with the $j$-th CFMM that has reserves $r^j$. When $r^j = d^j$ and $U_\\theta = U$, we have $y_\\Theta^j = \\tilde{y}_\\Theta^j$. However, the presence of noise forces these to differ.</p> <p>Below we overview the maths for the optimization algorithm. Then we define a PyTorch network that uses Davis-Yin Splitting (DYS) to compute inferences. Training is conducted by using Jacobian-Free Backprop (which maintains a low memory footprint by only backpropping through the final DYS iteration).</p> <p>This notebook accompanies the preprint Explainable AI via Learning to Optimize. In particular, see the appendices for more mathematical details about computations in this notebook.</p> <p>We emphasize the code herein is meant to be illustrative and, thus, has not be optimized for speed.</p>"},{"location":"exp-crypto/#levels-of-abstraction-in-math-formulation","title":"Levels of Abstraction in Math Formulation\u00b6","text":"<p>We begin at the highest level with the arbitrage problem</p> <p>$$ \\min_{(x,y)} \\sum_{j=1}^m \\left&lt; A^jp, A^j(x^j - y^j)\\right&gt;$$</p> <p>subject to the constraints</p> <p>$$ (x^j, y^j) = \\text{valid transaction with $j$-th CFMM with data $d^j$.}$$</p> <p>Here we assume CFMMs take use either a weighted arithmetic mean or weighted geometric mean for their constant function.  See our preprint for further detials on the constraints, their decoupling, and derivations of the proximal mappings for these. In our  setting, using the parameterization $\\xi=(v,x,y,z)$, we use the functions</p> <p>$$ f(\\xi;d) \\triangleq \\delta_{\\geq0}(x,y) + \\delta_{\\mathcal{M}}(v,z;d),$$</p> <p>$$ g(\\xi;d) \\triangleq \\delta_{\\mathcal{R}}(v,x,y) + \\delta_{\\mathcal{H}}(z;d),$$</p> <p>$$ h(\\xi;d) \\triangleq -U_\\Theta(x,y; d).$$</p> <p>The referenced indicator functions are given by</p> <p>$$ \\delta_{\\mathcal{M}}(v,z;d) \\triangleq \\sum_{i\\in\\mathcal{I}_1} \\delta_{\\mathcal{P}_j}(v^j, z^j;d) + \\sum_{j\\in\\mathcal{I}_2} \\delta_{\\mathcal{A}_j}(v^j;d),$$</p> <p>$$ \\delta_{\\mathcal{H}}(z) \\triangleq \\sum_{j\\in\\mathcal{I}_1} \\delta_{\\mathcal{H}_j} (z^j;d)$$</p> <p>$$ \\delta_{\\mathcal{R}}(v,x,y) \\triangleq \\sum_{j=1}^m \\delta_{\\mathcal{R}^j}(v^j,x^j,y^j),$$</p> <p>where $\\mathcal{I}_1$ is the set of all CFMM indices with weighted geometric means and $\\mathcal{I}_2 = [m] - \\mathcal{I}_1$ is the remaining indices for weighted arithmetic mean CFMMs, and</p> <p>$$ \\mathcal{P}_j \\triangleq \\{(v^j,z^j) : z^j \\leq \\ln(v^j + d) \\},$$</p> <p>$$ \\mathcal{A}_j \\triangleq \\{x^j : x+d^j \\geq 0,\\ \\left&lt;w^j,x^j\\right&gt; \\geq \\delta_j \\left&lt;w,r^j\\right&gt;,$$</p> <p>$$ \\mathcal{H}_j \\triangleq \\{z^j : \\left&lt;w^j,z^j\\right&gt; = \\ln(1-\\delta_j) + \\left&lt;w^j,  d^j\\right&gt;\\},$$</p> <p>$$ \\mathcal{R}_j \\triangleq \\{(v^j,x^j,y^j) : v^j = \\Gamma^j A^j x^j - A^j y^j \\}.$$</p> <p>Above $\\Gamma^j = \\text{diag}(\\gamma_j,\\ldots,\\gamma_j) \\in \\mathbb{R}^{n_j\\times n_j}$ gives the trade fees within a CFMM and $A^j \\in \\mathbb{R}^{n_j\\times n}$ is a binary matrix that maps global cryptoasset indices into the local coordinates of each CFMM.</p> <p>Below we provide a function for applying DYS to lists of tensors</p>"},{"location":"exp-crypto/#davis-yin-splitting","title":"Davis-Yin Splitting\u00b6","text":"<p>From the above parameterizations, the arbitrage problem above is equivalent to the minimization problem</p> <p>$$ \\min_{\\xi} f(\\xi) + g(\\xi) + h(\\xi), $$</p> <p>where $f$ and $g$ are proximable and $h$ is $L$-Lipschitz differentiable. Davis-Yin Splitting is applied, with $\\alpha \\in (0,2/L)$ via the iteration</p> <p>$$ \\xi^{k+1} = \\text{prox}_{\\alpha f}(\\zeta^k)$$ $$ \\psi^{k+1} = \\text{prox}_{\\alpha g}(2\\xi^{k+1}-\\zeta^k - \\alpha \\nabla h(\\xi^{k+1}))$$ $$ \\zeta^{k+1} = \\zeta^k + \\psi^{k+1}-\\xi^{k+1}$$</p> <p>Here</p> <p>$$ \\lim_{k\\rightarrow\\infty} \\xi^k = \\xi^\\star \\in \\text{argmin}_{\\xi} f(\\xi) + g(\\xi) + h(\\xi).$$</p>"},{"location":"exp-crypto/#cryptoasset-trade-utility","title":"Cryptoasset Trade Utility\u00b6","text":"<p>The differentiable function $h$ gives the trade utility, i.e.</p> <p>$$h(\\xi;d) \\triangleq  -U_\\Theta(x,y;d) = \\underbrace{\\sum_{j=1}^m \\left&lt;   A^j p,   A^j(x^j-y^j) \\right&gt;}_{\\text{net value lossed}} + \\underbrace{\\dfrac{\\psi_\\Theta(d)}{2}\\sum_{j=1}^m   \\left\\|W^j A^j (x^j - y^j)) \\right\\|^2 .}_{\\text{Risk term}}, $$</p> <p>Sigmoid... (CHECK LIPSCHITZ STUFF)</p> <p>$$ \\sigma(z) = \\dfrac{1}{1+\\exp(-z)}\\ \\ \\implies \\ \\ \\nabla \\sigma(z) = \\sigma(z)\\cdot(1-\\sigma(z)).$$</p> <p>$$  \\psi_\\Theta(d) \\cdot   A^\\top W^\\top WA(x-y) \\cdot \\text{diag}(\\sigma' (WA(x-y)+b)) $$</p> <p>where $\\Theta$ are weights consisting of $W$ and parameters for $\\psi_\\Theta:\\mathbb{R}^n\\rightarrow \\mathbb{R}$. The function $\\psi_\\Theta$ is   continuous, but is not (necessarily) convex in $d$. The oracle for utility takes the form of a gradient. To this end, set</p> <p>$$ W \\triangleq \\text{diag}(W^1,\\ldots, W^m),  \\ \\ \\ A \\triangleq \\text{diag}(A^1,\\ldots,A^m),$$</p> <p>and</p> <p>$$ S \\triangleq \\left[\\begin{array}{c} \\mathrm{I} \\\\ \\vdots \\\\ \\mathrm{I} \\end{array}\\right] \\in \\mathbb{R}^{mn\\times n}.$$</p> <p>Then, in full scale,</p> <p>$$ -U(x,y) = p^\\top S^\\top A^\\top A (x-y)   + \\dfrac{\\psi_\\Theta(d)}{2} \\| W A(x-y)\\|^2,$$</p> <p>which implies</p> <p>$$ -\\nabla_{x} U(x,y) = A^\\top ASp +  \\psi_\\Theta(d) A^\\top W^\\top  W A(x-y).$$</p> <p>Noting the flipped signs between $x$ and $y$, we see</p> <p>$$ \\nabla_{y} U(x,y) = -  \\nabla_{x} U(x,y).$$</p>"},{"location":"exp-crypto/#geometric-mean-constraints","title":"Geometric Mean Constraints\u00b6","text":"<p>Next we look at blockwise-projections onto the hyperplanes</p> <p>$$ \\mathcal{H}_j = \\left\\lbrace z^j : \\left&lt;w^j,z^j\\right&gt; = \\ln(1+\\delta_j) + \\ln\\left( \\left&lt;w^j, d^j\\right&gt;\\right)\\right\\rbrace, \\ \\ \\ \\text{for all $j\\in[m]$.}$$</p> <p>The formula for the $j$-th hyperplane projection is</p> <p>$$ P_{\\mathcal{H}_j}(z^j) = z^j - \\dfrac{\\left&lt;w^j,z^j\\right&gt;-b(d^j)}{\\|w^j\\|^2} w^j.$$</p> <p>where</p> <p>$$ b(d^j) = \\ln(1+\\delta_j) + \\left&lt;w, \\ln(d^j)\\right&gt;.$$</p> <p>Generalizing to all blocks gives the constraint</p> <p>$$ \\mathcal{H} = \\mathcal{H}_1 \\times \\mathcal{H}_2 \\times \\cdots \\times \\mathcal{H}_m .$$</p> <p>The projection on $\\mathcal{H}$ is given by</p> <p>$$ P_\\mathcal{H}(z) = z - N(z) w, $$</p> <p>where</p> <p>$$ N(z) \\triangleq \\text{diag}\\left( \\mu_1(z) \\mathrm{I}, \\ldots, \\mu_m(z) \\mathrm{I} \\right),$$</p> <p>$$\\delta_{jg} \\triangleq \\begin{cases}\\begin{array}{cl} 1 &amp; \\text{if $j$-th constraint is geometric}, \\\\ 0 &amp; \\text{otherwise,}\\end{array} \\end{cases}$$</p> <p>and</p> <p>$$ \\mu(z) \\triangleq \\text{diag}\\left(\\delta_{1g}\\|w^1\\|^{-2},\\ldots,\\delta_{mg}\\|w^m\\|^{-2}\\right)\\left[\\text{diag}\\left( (w^1)^\\top,\\ldots, (w^m)^\\top  \\right) \\cdot (z-\\ln(d)) - \\ln(1-\\delta)\\right].  $$</p> <p>Note the inclusion of $\\delta_{jg}$ means $\\mathcal{H}$ is not actually a tuple of just $\\mathcal{H}_j$ sets since some of these sets are simply all of $\\mathbb{R}^{n_j}$.</p>"},{"location":"exp-crypto/#halfspace-constraints","title":"Halfspace Constraints\u00b6","text":"<p>Next we look at projections on the set</p> <p>$$ \\mathcal{A}_j = \\left\\lbrace v : \\left&lt;w^j, v\\right&gt;  \\geq  \\delta_j \\left&lt;w^j, d^j\\right&gt; \\right\\rbrace.$$</p> <p>The formula is</p> <p>$$ P_{\\mathcal{A}_j}(v) = v - \\dfrac{\\left[\\left&lt;w^j,v\\right&gt; -  \\delta_j\\left&lt;w^j, d^j\\right&gt;\\right]_-}{\\|w^j\\|^2} w^j.$$</p> <p>Generalizing to all blocks gives the constraint</p> <p>$$ \\mathcal{A} = \\mathcal{A}_1 \\times \\mathcal{A}_2 \\times \\cdots \\times \\mathcal{A}_m .$$</p> <p>The projection on $\\mathcal{A}$ is given by</p> <p>$$ P_{\\mathcal{A}}(z) = z - N(z) w, $$</p> <p>where</p> <p>$$ N(z) \\triangleq \\text{diag}\\left( \\mu_1(z) \\mathrm{I}, \\ldots, \\mu_m(z) \\mathrm{I} \\right),$$</p> <p>$$\\delta_{ja} \\triangleq \\begin{cases}\\begin{array}{cl} 1 &amp; \\text{if $j$-th constraint is arithmetic}, \\\\ 0 &amp; \\text{otherwise,}\\end{array} \\end{cases}$$</p> <p>and</p> <p>$$ \\mu(z) \\triangleq \\left[ \\text{diag}\\left(\\delta_{1a}\\|w^1\\|^{-2},\\ldots,\\delta_{ma}\\|w^m\\|^{-2}\\right) \\cdot\\text{diag}\\left( (w^1)^\\top,\\ldots, (w^m)^\\top  \\right) \\cdot  (v+\\delta d)\\right]_-.  $$</p> <p>Note the inclusion of $\\delta_{jg}$ means $\\mathcal{A}$ is not actually a product of just $\\mathcal{A}_j$ sets since some of these sets are simply all of $\\mathbb{R}^{n_j}$.</p>"},{"location":"exp-crypto/#linear-system-constraints","title":"Linear System Constraints\u00b6","text":"<p>Next we consider the linear system $$ \\mathcal{R}_j \\triangleq \\{(v^j,x^j,y^j) : v^j = \\gamma_j A^j x^j - A^jy^j\\}, \\ \\ \\text{for all $j\\in[m]$.}$$</p> <p>Generalizing to all blocks gives the constraint</p> <p>$$ \\mathcal{R} \\triangleq \\mathcal{R_1}\\times\\mathcal{R_2}\\times\\cdots\\times \\mathcal{R_m} = \\{ (v,x,y) : v = \\Gamma Ax - Ay \\},$$</p> <p>where</p> <p>$$ A \\triangleq \\text{diag}(A^1,\\ldots, A^m) \\ \\ \\text{and} \\ \\ \\Gamma \\triangleq \\text{diag}(\\gamma_1 \\mathrm{I}, \\ldots, \\gamma_m \\mathrm{I}). $$</p> <p>Setting</p> <p>$$ N \\triangleq [\\Gamma A \\ \\ -A] \\ \\ \\text{and} \\ \\ M \\triangleq (\\mathrm{I}+N^\\top N)^{-1}$$</p> <p>yields</p> <p>$$ \\left[P_{R}(v,x,y)\\right]_{(x,y)} = M\\left[\\begin{array}{c} x+A^\\top \\Gamma v \\\\ y - A^\\top v\\end{array}\\right], $$</p> <p>from which the $v$ component is of the projection is directly obtained by matrix multiplication.</p>"},{"location":"exp-crypto/#logarithmic-constraints","title":"Logarithmic Constraints\u00b6","text":"<p>Lastly, we consider the set</p> <p>$$ \\mathcal{P} = \\{(v,z) : \\ln(v+d) \\geq z \\}.$$</p> <p>The projection $(\\overline{v},\\overline{z})$ of $(v,z)$ onto $\\mathcal{P}$ satisfies</p> <p>$$ 0 = (\\overline{v} + d) \\odot (\\overline{v} - {v})  + \\ln(\\overline{v}+d) - z.$$</p> <p>Since the relations are element-wise and separable, we compute the projection $(\\overline{x}, \\overline{z})$ using Newton iteration. Treating the right hand side like $f(\\hat{v})$ and differentiating $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ gives</p> <p>$$ f'(\\hat{v}) =  2\\hat{v} + (d-v) + \\dfrac{1}{\\hat{v}+d}.$$</p> <p>Then Newton iteration takes the form</p> <p>$$ \\hat{v}^{k+1} = \\hat{v}^k - \\dfrac{f(\\hat{v}^k)}{ f'(\\hat{v}^k)}.$$</p>"},{"location":"exp-crypto/#projection-onto-sum-of-constraints","title":"Projection onto sum of constraints\u00b6","text":"<p>Next we consider the set $\\mathcal{M}$, which is most easily expressed by using the sum of indicator functions, i.e.</p> <p>$$\\delta_{\\mathcal{M}}(\\xi) \\triangleq \\sum_{i\\in\\mathcal{I}_1} \\delta_{\\mathcal{P}_j}(v^j, z^j) + \\sum_{j\\in\\mathcal{I}_2} \\delta_{\\mathcal{A}_j}(v^j).$$</p> <p>This function marks a transition of input types. Now we use $\\xi = (v,x,y,z)$. For implementation, $\\xi$ is a list, i.e. $\\xi = [v, x, y, z]$ where each component is itself a list of tensors, one tensor for each CFMM.</p>"},{"location":"exp-crypto/#proximal-of-f","title":"Proximal of $f$\u00b6","text":"<p>Moving to the highest level of abstraction, we obtain</p> <p>$$f(\\xi) \\triangleq \\delta_{\\geq0}(x,y) + \\delta_{\\mathcal{M}}(v,z).$$</p> <p>Because the two terms of $f$ depend on different components of $\\xi$, we may apply the proximals sequentially.</p>"},{"location":"exp-crypto/#proximal-of-g","title":"Proximal of $g$\u00b6","text":"<p>The final function to compute proximals for is</p> <p>$$g(\\xi) \\triangleq \\delta_{\\mathcal{R}}(v,x,y) + \\delta_{\\mathcal{H}}(z).$$</p> <p>Since the components used by each of the two terms do not overlap, we can apply the proximal for each successively.</p>"},{"location":"exp-crypto/#pytests","title":"PyTests\u00b6","text":"<p>Various tests are used to check aspects of the above functions (mimicking what one usually encodes as pytests in a .py file).</p>"},{"location":"exp-crypto/#pytorch-model","title":"Pytorch Model\u00b6","text":""},{"location":"exp-crypto/#toy-crypto-trade-problem","title":"Toy Crypto Trade Problem\u00b6","text":""},{"location":"exp-crypto/#check-trade-violation","title":"Check trade violation\u00b6","text":"<p>This function is used to check whether a given trade is \"valid\".</p>"},{"location":"exp-crypto/#toy-problem","title":"Toy Problem\u00b6","text":"<p>The following snippet is used to show the implementation of the model (without training and without noise).</p>"},{"location":"exp-crypto/#price-generation","title":"Price Generation\u00b6","text":"<p>$$ \\dfrac{p_j}{p_k} = \\dfrac{\\nabla_j \\phi(d)}{\\nabla_k \\phi(d)}.$$</p> <p>For geometric CFMMs,</p> <p>$$ \\nabla_\\ell \\phi(d) = \\dfrac{w_\\ell}{d_\\ell} \\cdot \\phi(d).$$</p> <p>which implies</p> <p>$$ \\dfrac{p_j}{p_k} = \\dfrac{w_j / d_j}{w_k / d_k} = \\dfrac{w_j}{w_k}\\cdot \\dfrac{d_k}{d_j}.$$</p> <p>Thus, for a known prices $p_j$ and $p_k$, weights $w_j$ and $w_k$, and reserves $d_k$, we can find the amount of reserves for the $j$-th token; namely,</p> <p>$$ d_j = \\dfrac{w_j p_k d_k}{w_k p_j}.$$</p>"},{"location":"exp-crypto/#correct-trade-errors","title":"Correct Trade Errors\u00b6","text":"<p>Define</p> <p>$$ \\theta(\\tau) \\triangleq \\phi(d + \\Gamma A x - A(y - \\tau p)) - \\phi(d).$$</p> <p>Use Newton</p> <p>$$ \\tau_{k+1} = \\tau_k - \\dfrac{ \\theta(\\tau_k)}{\\theta'(\\tau_k)}.$$</p> <p>Let $q = d + \\gamma x - y$. For geommetric CFMMs, we use</p> <p>$$ \\tilde{\\phi}(v) \\triangleq \\left&lt; w, \\ln(v)\\right&gt;$$</p> <p>so that</p> <p>$$ \\theta'(\\tau) = \\left&lt; \\nabla \\tilde{\\phi}(z + \\tau A p),A p\\right&gt; = \\tilde{\\phi}(z+\\tau A p) \\left&lt;\\frac{w}{z+\\tau A p}, A p\\right&gt;.$$</p> <p>Meanwhile, for weighted arithmetic mean CFMMs,</p> <p>$$ \\tilde{\\phi}(v) \\triangleq \\left&lt; w, v\\right&gt;,$$</p> <p>for which</p> <p>$$ \\theta'(\\tau) = \\left&lt; \\nabla \\tilde{\\phi}(z + \\tau A p), Ap\\right&gt; =  \\left&lt;w, Ap\\right&gt;.$$</p>"},{"location":"exp-crypto/#train-model","title":"Train Model\u00b6","text":""},{"location":"exp-crypto/#testing-results","title":"Testing Results\u00b6","text":""},{"location":"exp-crypto/#plot-data-for-figure-10-in-paper","title":"Plot Data for Figure 10 in paper\u00b6","text":""},{"location":"exp-ct/","title":"Implicit Deep Learning for CT","text":"<p>Herein we overview the model and setup for the CT image reconstruction experiments.</p>"},{"location":"exp-ct/#ct-data","title":"CT Data","text":"<p>The datasets used in this set of experiments are stored in a publicly accesible Google Drive folder.</p> <p> Download CT Data </p> <p></p>"},{"location":"exp-ct/#ct-model-overview","title":"CT Model Overview","text":"<p>               Bases: <code>ImplicitL2OModel</code></p> <p>Model to reconstruct CT image from measurements.</p> <p>Inferences are defined by</p> <pre><code>model(d) = argmin f_theta(Kx)   s.t.   ||Ax - d|| &lt; delta,\n</code></pre> <p>where K, theta, and delta are tunable parameters. The forward iteration is Linearized ADMM (L-ADMM) and the stepsizes in the algorithm are tunable too.</p> <p></p>"},{"location":"exp-ct/#apply-optimization-step","title":"Apply Optimization Step","text":"<p>Apply model operator using L-ADMM update</p> <p>Core functionality is single iteration update for Linearized ADMM, which is rearranged to make the signal 'u' update last. This is needed to ensure the JFB attaches gradients.</p> Source code in <code>src/models.py</code> <pre><code>def _apply_T(self, x: inference, d: input_data, return_tuple=False):\n    ''' Apply model operator using L-ADMM update\n\n        Core functionality is single iteration update for Linearized ADMM,\n        which is rearranged to make the signal 'u' update last. This is\n        needed to ensure the JFB attaches gradients.\n    '''\n    batch_size = x.shape[0]\n\n    d = d.view(d.shape[0], -1).to(self.device())\n    d = d.permute(1, 0)\n    xk = x.view(x.shape[0], -1)\n    xk = xk.permute(1, 0)\n    pk = self.K(xk)\n    wk = torch.matmul(self.A, xk)\n    nuk1 = torch.zeros(pk.size(), device=self.device())\n    nuk2 = torch.zeros(d.size(), device=self.device())\n\n    alpha = torch.clamp(self.alpha.data, min=0, max=2)\n    beta = torch.clamp(self.beta.data, min=0, max=2)\n    lambd = torch.clamp(self.lambd.data, min=0, max=2)\n    delta = self.delta.data\n\n    # pk step\n    pk = pk + lambd*(nuk1 + alpha * (self.K(xk) - pk))\n    pk = self.R(pk)\n\n    # wk step\n    Axk = torch.matmul(self.A, xk)\n    res_temp = nuk2 + alpha * (Axk - wk)\n    temp_term = wk + lambd * res_temp\n    # temp_term = self.S(wk + lambd * res_temp)\n    wk = self.ball_proj(temp_term, d, delta)\n\n    # nuk1 step\n    res_temp = self.K(xk) - pk\n    nuk1_plus = nuk1 + alpha * res_temp\n\n    # nuk2 step\n    res_temp = Axk - wk\n    nuk2_plus = nuk2 + alpha * res_temp\n\n    # rk step\n    self.convK_T.weight.data = self.convK.weight.data\n    rk = self.Kt(2*nuk1_plus - nuk1)\n    rk = rk + torch.matmul(self.At, 2*nuk2_plus - nuk2)\n\n    # xk step\n    xk = torch.clamp(xk - beta * rk, min=0, max=1)\n\n    if return_tuple:\n        return xk.permute(1, 0).view(batch_size, 1, 128, 128), nuk1_plus, pk\n    else:\n        return xk.permute(1, 0).view(batch_size, 1, 128, 128)\n</code></pre> <p></p>"},{"location":"exp-ct/#get-convergence-criteria","title":"Get Convergence Criteria","text":"<p>Identify criteria for whether forward iteration to converges</p> <p>Criteria implies update residual should be small for x, i.e. the        expression |x^{k+1} - x^k|| is close to zero</p> Source code in <code>src/models.py</code> <pre><code>def _get_conv_crit(self, x, x_prev, d, tol=1.0e-2):\n    ''' Identify criteria for whether forward iteration to converges\n\n        Criteria implies update residual should be small for x, i.e. the\n               expression |x^{k+1} - x^k|| is close to zero\n    '''\n    batch_size = x.shape[0]\n    x = x.view(batch_size, -1)\n    d = d.view(batch_size, -1)\n    x_prev = x_prev.view(batch_size, -1)\n\n    res_norm = torch.max(torch.norm(x - x_prev, dim=1))\n    residual_conv = res_norm &lt;= tol\n\n    return residual_conv\n</code></pre> <p></p>"},{"location":"exp-ct/#forward","title":"Forward","text":"<p>Compute inference using L-ADMM.</p> <p>The aim is to find nu = T(nu; d) where nu is the dual variable for minimization problem, and T is the update operation for L-ADMM. Associated with optimal dual, we obtain the inference u.</p> Source code in <code>src/models.py</code> <pre><code>def forward(self, d, depth_warning=False, return_depth=False, tol=1e-3, return_all_vars=False):\n    ''' Compute inference using L-ADMM.\n\n        The aim is to find nu* = T(nu*; d) where nu* is the dual variable\n        for minimization problem, and T is the update operation for L-ADMM.\n        Associated with optimal dual, we obtain the inference u*.\n    '''\n    with torch.no_grad():\n\n        self.depth = 0.0\n        x = torch.zeros((d.size()[0], 1, 128, 128),\n                        device=self.device())\n        x_prev = np.Inf*torch.ones(x.shape, device=self.device())\n        all_samp_conv = False\n\n        while not all_samp_conv and self.depth &lt; self.max_depth:\n            x_prev = x.clone()\n            x = self._apply_T(x, d)\n            all_samp_conv = self._get_conv_crit(x,\n                                                x_prev,\n                                                d,\n                                                tol=tol)\n\n            self.depth += 1\n\n    if self.depth &gt;= self.max_depth and depth_warning:\n        print(\"\\nWarning: Max Depth Reached - Break Forward Loop\\n\")\n\n    self.fixed_point_error = torch.max(torch.norm(x - x_prev, dim=1))\n\n    if return_depth and return_all_vars==False:\n        Tx = self._apply_T(x, d)\n        return Tx, self.depth\n    elif return_all_vars:\n        Tx, nuk1, pk = self._apply_T(x, d, return_tuple=True)\n        return Tx, nuk1, pk\n    else:\n        Tx = self._apply_T(x, d)\n        return Tx\n</code></pre>"},{"location":"exp-dictionary/","title":"Implicit Dictionary Learning","text":"<p>This is where we overview the model for the toy experiment.</p> <p>Full Tutorial</p> <p>See the tutorial page for a Jupyter notebook using this model.  </p>"},{"location":"exp-dictionary/#model-overview","title":"Model Overview","text":"<p>               Bases: <code>ImplicitL2OModel</code></p> <p>Model to recover signal from measurements by leveraging sparse structure</p> <p>Inferences are defined by</p> \\[     \\mathsf{model(d) = argmin_x\\ \\| Kx \\|_1  \\quad s.t. \\quad Ax = d,} \\] <p>where \\(\\mathsf{K}\\) is a tunable matrix. Because the model is equivalent under scaling of \\(\\mathsf{K}\\), we fix \\(\\mathsf{|| K ||_2 = 1}\\). This is enforced during training by dividing by the matrix norm at the beginning of forward propagation. The forward iteration is Linearized ADMM (L-ADMM).</p> <p></p>"},{"location":"exp-dictionary/#apply-optimization-step","title":"Apply Optimization Step","text":"<p>Apply model operator using L-ADMM update</p> <p>Core functionality is single iteration update for Linearized ADMM, which is rearranged to make the signal \\(\\mathsf{x}\\) update last. This is needed to ensure the JFB backprop attaches gradients. Here the tuple \\(\\mathsf{(\\hat{x}, \\hat{p}, \\hat{v}_1, \\hat{v}_2)}\\) is given as input. Each update is given by the following.</p> <p>\\(\\mathsf{p \\leftarrow shrink(\\hat{p} + {\\lambda} (\\hat{v}_1 + a (K\\hat{x} - \\hat{p})))}\\)</p> <p>\\(\\mathsf{v_1 \\leftarrow \\hat{v}_1 + \\alpha (K\\hat{x} - p)}\\)</p> <p>\\(\\mathsf{v_2 \\leftarrow \\hat{v}_2 + \\alpha (Ax - d)}\\)</p> <p>\\(\\mathsf{r \\leftarrow  K^\\top (2v_1 - \\hat{v}_1) + A^\\top (2v_2 - \\hat{v}_2)}\\)</p> <p>\\(\\mathsf{x \\leftarrow x - {\\beta} r}\\)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tensor</code> <p>Signal Estimate</p> required <code>p</code> <code>tensor</code> <p>Sparse transform \\(\\mathsf{Kx}\\) of signal</p> required <code>v1</code> <code>tensor</code> <p>Dual variable for sparsity transform constraint \\(\\mathsf{Kx=p}\\)</p> required <code>v2</code> <code>tensor</code> <p>Dual variable for linear constraint</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>tensor</code> <p>Updated Signal</p> Source code in <code>src/models.py</code> <pre><code>def _apply_T(self, x: inference, p: torch.tensor, v1: dual, v2: dual,\n             d: input_data, return_tuple=False) -&gt; inference:\n    r\"\"\" Apply model operator using L-ADMM update\n\n        Core functionality is single iteration update for Linearized ADMM,\n        which is rearranged to make the signal $\\mathsf{x}$ update last.\n        This is needed to ensure the JFB backprop attaches gradients. Here\n        the tuple $\\mathsf{(\\hat{x}, \\hat{p}, \\hat{v}_1, \\hat{v}_2)}$ is\n        given as input. Each update is given by the following.\n\n        $\\mathsf{p \\leftarrow shrink(\\hat{p} + {\\lambda} (\\hat{v}_1\n        + a (K\\hat{x} - \\hat{p})))}$\n\n        $\\mathsf{v_1 \\leftarrow \\hat{v}_1 + \\alpha (K\\hat{x} - p)}$\n\n        $\\mathsf{v_2 \\leftarrow \\hat{v}_2 + \\alpha (Ax - d)}$\n\n        $\\mathsf{r \\leftarrow  K^\\top (2v_1 - \\hat{v}_1)\n        + A^\\top (2v_2 - \\hat{v}_2)}$\n\n        $\\mathsf{x \\leftarrow x - {\\beta} r}$\n\n        Args:\n            x (tensor): Signal Estimate\n            p (tensor): Sparse transform $\\mathsf{Kx}$ of signal\n            v1 (tensor): Dual variable for sparsity transform constraint $\\mathsf{Kx=p}$\n            v2 (tensor): Dual variable for linear constraint\n\n        Returns:\n            x (tensor): Updated Signal\n    \"\"\"\n    x  = x.permute(1, 0).float()\n    p  = p.permute(1, 0).float()\n    d  = d.permute(1, 0).float()\n    v1 = v1.permute(1, 0).float()\n    v2 = v2.permute(1, 0).float()\n\n    Kx = torch.mm(self.K.float(), x)\n    Ax = torch.mm(self.A.to(device=self.device()), x)\n    p  = self.shrink(p + self.lambd * (v1 + self.alpha * (Kx - p)))\n\n    v1_prev = v1.to(self.device())\n    v2_prev = v2.to(self.device())\n\n    v1 = v1 + self.alpha * (Kx - p)\n    v2 = v2 + self.alpha * (Ax - d)\n    r  = self.K.t().mm(2 * v1 - v1_prev).to(self.device())\n    r += self.A.to(device=self.device()).t().mm(2 * v2 - v2_prev)\n    x  = x - self.beta * r\n\n    x  = x.permute(1, 0)\n    p  = p.permute(1, 0)\n    v1 = v1.permute(1, 0)\n    v2 = v2.permute(1, 0)\n    if return_tuple:\n        return x, p, v1, v2\n    else:\n        return x\n</code></pre> <p></p>"},{"location":"exp-dictionary/#get-convergence-criteria","title":"Get Convergence Criteria","text":"<p>Identify criteria for whether forward iteration to converges</p> Convergence Criteria <ol> <li>Fidelity must satisfy \\(\\mathsf{\\| Ax - d\\| \\leq tol \\cdot ||d||}\\)</li> <li>Update residual should be small for x and v, i.e. the    expression \\(\\mathsf{\\|x^{k+1} - x^k|| + ||v^{k+1} - v^k||}\\) is close    to zero relative to \\(\\mathsf{\\|x^k\\| + \\|v^k\\|}\\).</li> </ol> Note <p>Tolerance is added to <code>norm_data</code> to handle the case where \\(\\mathsf{d = 0}\\).</p> Source code in <code>src/models.py</code> <pre><code>def _get_conv_crit(self, x, x_prev, v1, v1_prev, v2, v2_prev, d,\n                   tol_fidelity=1.0e-2, tol_residual=1.0e-4,\n                   tol_num_stability=1.0e-8):\n    \"\"\" Identify criteria for whether forward iteration to converges\n\n        Convergence Criteria:\n            1. Fidelity must satisfy $\\mathsf{\\| Ax - d\\| \\leq tol \\cdot ||d||}$\n            2. Update residual should be small for x and v, i.e. the\n               expression $\\mathsf{\\|x^{k+1} - x^k|| + ||v^{k+1} - v^k||}$ is close\n               to zero relative to $\\mathsf{\\|x^k\\| + \\|v^k\\|}$.\n\n        Note:\n            Tolerance is added to `norm_data` to handle the case where\n            $\\mathsf{d = 0}$.\n    \"\"\"\n    norm_res = torch.max(torch.norm(v1 - v1_prev, dim=1))\n    norm_res += torch.max(torch.norm(v2 - v2_prev, dim=1))\n    norm_res += torch.max(torch.norm(x - x_prev, dim=1))\n\n    norm_res_ref = torch.max(torch.norm(x_prev, dim=1))\n    norm_res_ref += torch.max(torch.norm(v1_prev, dim=1))\n    norm_res_ref += torch.max(torch.norm(v2_prev, dim=1))\n\n    fidelity = torch.mm(x, self.A.t().to(self.device())) - d\n    norm_fidelity = torch.min(torch.norm(fidelity, dim=1))\n\n    norm_data = torch.max(torch.norm(d, dim=1))\n    norm_data += tol_num_stability * norm_fidelity\n\n    residual_conv = norm_res &lt;= tol_residual * norm_res_ref\n    feasible_sol = norm_fidelity &lt;= tol_fidelity * norm_data\n    return residual_conv and feasible_sol\n</code></pre> <p></p>"},{"location":"exp-dictionary/#forward","title":"Forward","text":"<p>Compute inference using L-ADMM.</p> <p>The aim is to find \\(\\mathsf{v^\\star}\\) satisfying \\(\\mathsf{v^\\star = T(v^\\star; d)}\\)  where \\(\\mathsf{v^\\star}\\) is the dual variable for minimization problem,  and \\(\\mathsf{T}\\) is the update operation for L-ADMM. This operation is applied repeatedly until an approximate fixed point of \\(\\mathsf{T(\\cdot; d)}\\) is found. Associated with optimal dual, we obtain the inference x*.</p> Note <p>We write the dual as a tuple \\(\\mathsf{v = (v_1, v_2)}\\).</p> Source code in <code>src/models.py</code> <pre><code>def forward(self, d: input_data, max_depth=5000,\n            depth_warning=False, return_depth=False,\n            normalize_K=False, return_certs=False) -&gt; inference:\n    \"\"\" Compute inference using L-ADMM.\n\n        The aim is to find $\\mathsf{v^\\star}$ satisfying $\\mathsf{v^\\star = T(v^\\star; d)}$ \n        where $\\mathsf{v^\\star}$ is the dual variable for minimization problem, \n        and $\\mathsf{T}$ is the update operation for L-ADMM. This operation is applied\n        repeatedly until an approximate fixed point of $\\mathsf{T(\\cdot; d)}$ is found.\n        Associated with optimal dual, we obtain the inference x*.\n\n        Note:\n            We write the dual as a tuple $\\mathsf{v = (v_1, v_2)}$.\n    \"\"\"\n    self.A = self.A.to(self.device())\n    d = d.to(self.device()).float()\n\n    with torch.no_grad():\n        if normalize_K:\n            K_norm = torch.linalg.matrix_norm(self.K, ord=2)\n            self.K /= K_norm\n\n        self.depth = 0.0\n        x_size = self.A.size()[1]\n        d_size = self.A.size()[0]\n\n        x = torch.zeros((d.size()[0], x_size),\n                        device=self.device(), dtype=float)\n        p = torch.zeros((d.size()[0], x_size),\n                        device=self.device(), dtype=float)\n        v1 = torch.zeros((d.size()[0], x_size),\n                         device=self.device(), dtype=float)\n        v2 = torch.zeros((d.size()[0], d_size),\n                         device=self.device(), dtype=float)\n\n        x_prev = x.clone()\n        all_samp_conv = False\n        while not all_samp_conv and self.depth &lt; max_depth:\n            v1_prev = v1.clone()\n            v2_prev = v2.clone()\n            x_prev = x.clone()\n\n            x, p, v1, v2 = self._apply_T(x, p, v1, v2, d,\n                                         return_tuple=True)\n            all_samp_conv = self._get_conv_crit(x, x_prev, v1, v1_prev,\n                                                v2, v2_prev, d)\n\n            self.depth += 1\n\n    if self.depth &gt;= max_depth and depth_warning:\n        print(\"\\nWarning: Max Depth Reached - Break Forward Loop\\n\")\n\n    Tx = self._apply_T(x, p, v1, v2, d)\n    output = [Tx]\n    if return_depth:\n        output.append(self.depth)\n    if return_certs:\n        output.append(self.get_certs(Tx.detach(), d))\n    return output if len(output) &gt; 1 else Tx\n</code></pre>"},{"location":"tutorial-toy-exp/","title":"Training Toy Experiment","text":"In\u00a0[\u00a0]: Copied! <pre>def func(x):\n    sum = 0\n    for i in range(3):\n        sum += i * x\n    return sum\n</pre> def func(x):     sum = 0     for i in range(3):         sum += i * x     return sum"},{"location":"tutorial-toy-exp/#training-toy-experiment","title":"Training Toy Experiment\u00b6","text":"<p>This is a test notebook. $$ \\int_0^t f(\\tau) \\ \\text{d}\\tau = 7.$$</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>First, we import various utilities and mount Google drive (where this notebook was executed).</p> In\u00a0[1]: Copied! <pre>import os\nimport sys\nfrom google.colab import drive\ndrive.mount('/content/drive')\nsys.path.append('/content/drive/MyDrive/xai-l2o/src/')\nsave_dir = './drive/MyDrive/xai-l2o/'\n\n# from certificate import CertificateModel, CertificateEnsemble\nfrom utils import solve_least_squares, create_dict_loaders\nfrom utils import plot_dict_signal, print_model_params\nfrom models import ImpDictModel\nimport scipy.io\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data.dataset import random_split\n\ntorch.manual_seed(31415)\n\nloader_train, loader_test, A = create_dict_loaders()\n\nmax_epoch = 75\ndevice    = 'cuda:0'\nmodel     = ImpDictModel(A)\nmodel     = model.to(device=device)\ncriterion = nn.MSELoss()\nfile_name = save_dir + 'weights/dictionary_model_weights.pth'\n\nloss_best       = 1.0e10\nMSE_ave         = 0.0\nlearning_rate   = 4.0e-5\nmax_depth_train = 400\nmax_depth_test  = 2000\noptimizer       = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1.0e-5)\ntraining_msg    = '[{:5d}] train loss = {:2.3e} | depth = {:3.0f} | lr = {:2.3e}'\ntraining_msg   += ' | K 2-norm = {:2.3e}'\n\nmodel.to(device)\n\nprint_model_params(model)\n\nload_weights = False\nif load_weights:\n    state = torch.load(file_name, map_location=torch.device(device))\n    model.load_state_dict(state['model_state_dict'])\n    print('Loaded model from file.')\n    epochs_adm = 0\n</pre> import os import sys from google.colab import drive drive.mount('/content/drive') sys.path.append('/content/drive/MyDrive/xai-l2o/src/') save_dir = './drive/MyDrive/xai-l2o/'  # from certificate import CertificateModel, CertificateEnsemble from utils import solve_least_squares, create_dict_loaders from utils import plot_dict_signal, print_model_params from models import ImpDictModel import scipy.io import numpy as np import torch import torch.nn as nn from torch.utils.data import TensorDataset, DataLoader from torch.utils.data.dataset import random_split  torch.manual_seed(31415)  loader_train, loader_test, A = create_dict_loaders()  max_epoch = 75 device    = 'cuda:0' model     = ImpDictModel(A) model     = model.to(device=device) criterion = nn.MSELoss() file_name = save_dir + 'weights/dictionary_model_weights.pth'  loss_best       = 1.0e10 MSE_ave         = 0.0 learning_rate   = 4.0e-5 max_depth_train = 400 max_depth_test  = 2000 optimizer       = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1.0e-5) training_msg    = '[{:5d}] train loss = {:2.3e} | depth = {:3.0f} | lr = {:2.3e}' training_msg   += ' | K 2-norm = {:2.3e}'  model.to(device)  print_model_params(model)  load_weights = False if load_weights:     state = torch.load(file_name, map_location=torch.device(device))     model.load_state_dict(state['model_state_dict'])     print('Loaded model from file.')     epochs_adm = 0 <pre>Mounted at /content/drive\n+-------------------+--------------+\n| Network Component | # Parameters |\n+-------------------+--------------+\n|         K         |    62500     |\n|       TOTAL       |    62500     |\n+-------------------+--------------+\n</pre> In\u00a0[2]: Copied! <pre>for epoch in range(max_epoch):\n    model.train()\n    for x_true, d_batch in loader_train:\n        optimizer.zero_grad()\n        x_pred, depth = model(d_batch, max_depth=max_depth_train,\n                              normalize_K=True, return_depth=True)\n        loss = criterion(x_pred, x_true.to(device).float())\n        loss.backward()\n        optimizer.step()\n        loss_curr = loss.detach().item()\n\n    if epoch % 10 == 0:\n        plot_dict_signal(model, loader_test, inference_depth=max_depth_test)\n\n    if loss_curr &lt; loss_best:\n        loss_best = loss_curr\n        state = {\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }\n        torch.save(state, file_name)\n        print('Model weights saved to ' + file_name)\n\n    K_norm = torch.linalg.matrix_norm(model.K.detach(), ord=2)\n    print(training_msg.format(epoch, loss_curr, depth,\n                              optimizer.param_groups[0]['lr'], K_norm))\n</pre> for epoch in range(max_epoch):     model.train()     for x_true, d_batch in loader_train:         optimizer.zero_grad()         x_pred, depth = model(d_batch, max_depth=max_depth_train,                               normalize_K=True, return_depth=True)         loss = criterion(x_pred, x_true.to(device).float())         loss.backward()         optimizer.step()         loss_curr = loss.detach().item()      if epoch % 10 == 0:         plot_dict_signal(model, loader_test, inference_depth=max_depth_test)      if loss_curr &lt; loss_best:         loss_best = loss_curr         state = {             'model_state_dict': model.state_dict(),             'optimizer_state_dict': optimizer.state_dict()         }         torch.save(state, file_name)         print('Model weights saved to ' + file_name)      K_norm = torch.linalg.matrix_norm(model.K.detach(), ord=2)     print(training_msg.format(epoch, loss_curr, depth,                               optimizer.param_groups[0]['lr'], K_norm)) <pre>Model weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    0] train loss = 4.517e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    1] train loss = 3.454e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    2] train loss = 2.508e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    3] train loss = 2.057e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    4] train loss = 1.678e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[    5] train loss = 1.681e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    6] train loss = 1.224e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    7] train loss = 1.100e+00 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[    8] train loss = 7.953e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[    9] train loss = 8.571e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> <pre>Model weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   10] train loss = 7.052e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   11] train loss = 5.494e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   12] train loss = 5.340e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   13] train loss = 4.789e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   14] train loss = 4.106e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   15] train loss = 3.093e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   16] train loss = 2.922e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   17] train loss = 2.745e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   18] train loss = 2.305e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   19] train loss = 1.662e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> <pre>Model weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   20] train loss = 1.486e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   21] train loss = 1.426e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   22] train loss = 1.246e-01 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   23] train loss = 9.714e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   24] train loss = 8.895e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   25] train loss = 8.003e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   26] train loss = 6.286e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   27] train loss = 5.849e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   28] train loss = 5.447e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   29] train loss = 4.025e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> <pre>Model weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   30] train loss = 2.939e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   31] train loss = 2.802e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   32] train loss = 2.250e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   33] train loss = 2.195e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   34] train loss = 1.735e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   35] train loss = 1.775e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   36] train loss = 1.341e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   37] train loss = 1.308e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   38] train loss = 1.020e-02 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   39] train loss = 9.429e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> <pre>Model weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   40] train loss = 8.157e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   41] train loss = 7.205e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   42] train loss = 7.407e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   43] train loss = 5.772e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   44] train loss = 4.510e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   45] train loss = 4.862e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   46] train loss = 4.249e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   47] train loss = 3.728e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   48] train loss = 4.024e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   49] train loss = 3.714e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> <pre>Model weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   50] train loss = 3.396e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   51] train loss = 3.416e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   52] train loss = 3.149e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   53] train loss = 3.605e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   54] train loss = 3.377e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   55] train loss = 3.221e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   56] train loss = 3.386e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   57] train loss = 3.252e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   58] train loss = 2.867e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   59] train loss = 2.898e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> <pre>[   60] train loss = 2.959e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   61] train loss = 3.224e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   62] train loss = 3.140e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   63] train loss = 3.102e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   64] train loss = 3.085e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   65] train loss = 2.834e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   66] train loss = 2.826e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   67] train loss = 2.889e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   68] train loss = 2.925e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\nModel weights saved to ./drive/MyDrive/xai-l2o/weights/dictionary_model_weights.pth\n[   69] train loss = 2.720e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> <pre>[   70] train loss = 2.849e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   71] train loss = 2.727e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   72] train loss = 3.067e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   73] train loss = 2.760e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n[   74] train loss = 2.825e-03 | depth = 400 | lr = 4.000e-05 | K 2-norm = 1.000e+00\n</pre> In\u00a0[3]: Copied! <pre>def get_sparsity(x, d=None) -&gt; float:\n    \"\"\" Quantify sparsity of signal via number of nonzeros\n\n        Signals are expected to be sparse asymptotically, i.e. if\n        the forward prop runs for infinitely many steps (due to the\n        linear coupling between variables 'Kx' and 'p' in the model).\n        Sparsity is approximately quantified by the L1 norm.\n\n        Note: We use the transpose of Kx throughout due to shape of x.\n    \"\"\"\n    Kx = model.K.detach().cpu().mm(x)\n    return float(torch.norm(Kx, dim=0, p=1))\n\n\ndef get_fidelity(x, d, tol_norm=1.0e-10) -&gt; float:\n    \"\"\" Compute norm of relative error for linear measurements\n    \"\"\"\n    fidelity = A.cpu().mm(x) - d.cpu()\n    norm_fidelity = torch.norm(fidelity, dim=0)\n    norm_data = torch.norm(d.cpu(), dim=0)\n    norm_data += tol_norm * norm_fidelity\n    norm_fidelity_rel = norm_fidelity / norm_data\n    return float(norm_fidelity_rel)\n</pre> def get_sparsity(x, d=None) -&gt; float:     \"\"\" Quantify sparsity of signal via number of nonzeros          Signals are expected to be sparse asymptotically, i.e. if         the forward prop runs for infinitely many steps (due to the         linear coupling between variables 'Kx' and 'p' in the model).         Sparsity is approximately quantified by the L1 norm.          Note: We use the transpose of Kx throughout due to shape of x.     \"\"\"     Kx = model.K.detach().cpu().mm(x)     return float(torch.norm(Kx, dim=0, p=1))   def get_fidelity(x, d, tol_norm=1.0e-10) -&gt; float:     \"\"\" Compute norm of relative error for linear measurements     \"\"\"     fidelity = A.cpu().mm(x) - d.cpu()     norm_fidelity = torch.norm(fidelity, dim=0)     norm_data = torch.norm(d.cpu(), dim=0)     norm_data += tol_norm * norm_fidelity     norm_fidelity_rel = norm_fidelity / norm_data     return float(norm_fidelity_rel) In\u00a0[4]: Copied! <pre># create a list for everything\nval_sparse = []\n\nprint('creating list')\nfor x_true, d_batch in loader_train:\n    optimizer.zero_grad()\n    x_pred = model(d_batch, max_depth=max_depth_test).cpu()\n    n      = x_pred.shape[1]\n\n    for i in range(x_pred.shape[0]):\n        l1 = get_sparsity(torch.reshape(x_pred[i, :], (n, 1)))\n        val_sparse.append(l1)\nprint('list created')\nval_sparse.sort()\n\n# fail threshold = 5%\nindex = int((1.0 - 0.05) * len(val_sparse))\nprint(\"index = \", index)\nprint(\"thresh = \", val_sparse[index])\nthresh = val_sparse[index]\n</pre> # create a list for everything val_sparse = []  print('creating list') for x_true, d_batch in loader_train:     optimizer.zero_grad()     x_pred = model(d_batch, max_depth=max_depth_test).cpu()     n      = x_pred.shape[1]      for i in range(x_pred.shape[0]):         l1 = get_sparsity(torch.reshape(x_pred[i, :], (n, 1)))         val_sparse.append(l1) print('list created') val_sparse.sort()  # fail threshold = 5% index = int((1.0 - 0.05) * len(val_sparse)) print(\"index = \", index) print(\"thresh = \", val_sparse[index]) thresh = val_sparse[index] <pre>creating list\nlist created\nindex =  9500\nthresh =  28.128530502319336\n</pre> In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nplt.hist(val_sparse, bins=30, color='skyblue', edgecolor='black')\nplt.xlabel('L1 Norm')\nplt.ylabel('Frequency')\nplt.title('Histogram for Sparsity of Training Inferences')\nplt.show()\n</pre> import matplotlib.pyplot as plt plt.hist(val_sparse, bins=30, color='skyblue', edgecolor='black') plt.xlabel('L1 Norm') plt.ylabel('Frequency') plt.title('Histogram for Sparsity of Training Inferences') plt.show() In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nfrom prettytable import PrettyTable\n\nm = A.shape[0]\nn = A.shape[1]\n\nx_true, d_true = list(loader_test)[0]\nn              = x_true.shape[1]\nx_wrong, _     = list(loader_test)[0]\nx_true         = torch.reshape(x_true[0,:].cpu(), (n, 1))\nx_wrong        = torch.reshape(x_wrong[1,:], (n, 1))\nx_pred         = model(d_true.to(device), max_depth=max_depth_test).cpu()[0, :]\nx_pred         = torch.reshape(x_pred, (n, 1))\nx_ls           = solve_least_squares(A.cpu(), d_true.cpu())\nx_ls           = torch.reshape(x_ls[0, :], (n,1))\nd_true         = torch.reshape(d_true[0,:],  (m, 1))\n\ntable      = PrettyTable([\"Method\", \"Property\", \"Flag\", \"Value\"])\nproperties = [{'name': 'sparsity', 'thresh': thresh, 'func': get_sparsity},\n              {'name': 'fidelity', 'thresh': 0.010, 'func': get_fidelity}]\nmethods    = [{'name': 'true', 'vec': x_true},\n              {'name': 'wrong', 'vec': x_wrong},\n              {'name': 'least_squares', 'vec': x_ls},\n              {'name': 'pred', 'vec': x_pred}]\nfig, axes  = plt.subplots(4, 1, sharex=\"col\", figsize=(7, 9))\nt          = np.linspace(1, n, n, endpoint=True)\n\nfor idx, method in enumerate(methods):\n    for prop in properties:\n        x         = method['vec'].cpu()\n        prop_val  = prop['func'](x, d=d_true)\n        prop_pass = \"Pass\" if (prop_val &lt; prop['thresh']) else \"Fail\"\n        table.add_row([method['name'], prop['name'],  prop_pass, prop_val])\n\n    Kx = model.K.detach().cpu() @ x\n    axes[idx].plot(t, Kx.detach().numpy())\n    axes[idx].set_title(method['name'])\n\nprint(table)\n\nfig.suptitle(\"Sparsified Outputs (i.e. pre-multiplied by K)\", fontsize=16)\nplt.show()\n</pre> import matplotlib.pyplot as plt from prettytable import PrettyTable  m = A.shape[0] n = A.shape[1]  x_true, d_true = list(loader_test)[0] n              = x_true.shape[1] x_wrong, _     = list(loader_test)[0] x_true         = torch.reshape(x_true[0,:].cpu(), (n, 1)) x_wrong        = torch.reshape(x_wrong[1,:], (n, 1)) x_pred         = model(d_true.to(device), max_depth=max_depth_test).cpu()[0, :] x_pred         = torch.reshape(x_pred, (n, 1)) x_ls           = solve_least_squares(A.cpu(), d_true.cpu()) x_ls           = torch.reshape(x_ls[0, :], (n,1)) d_true         = torch.reshape(d_true[0,:],  (m, 1))  table      = PrettyTable([\"Method\", \"Property\", \"Flag\", \"Value\"]) properties = [{'name': 'sparsity', 'thresh': thresh, 'func': get_sparsity},               {'name': 'fidelity', 'thresh': 0.010, 'func': get_fidelity}] methods    = [{'name': 'true', 'vec': x_true},               {'name': 'wrong', 'vec': x_wrong},               {'name': 'least_squares', 'vec': x_ls},               {'name': 'pred', 'vec': x_pred}] fig, axes  = plt.subplots(4, 1, sharex=\"col\", figsize=(7, 9)) t          = np.linspace(1, n, n, endpoint=True)  for idx, method in enumerate(methods):     for prop in properties:         x         = method['vec'].cpu()         prop_val  = prop['func'](x, d=d_true)         prop_pass = \"Pass\" if (prop_val &lt; prop['thresh']) else \"Fail\"         table.add_row([method['name'], prop['name'],  prop_pass, prop_val])      Kx = model.K.detach().cpu() @ x     axes[idx].plot(t, Kx.detach().numpy())     axes[idx].set_title(method['name'])  print(table)  fig.suptitle(\"Sparsified Outputs (i.e. pre-multiplied by K)\", fontsize=16) plt.show() <pre>+---------------+----------+------+------------------------+\n|     Method    | Property | Flag |         Value          |\n+---------------+----------+------+------------------------+\n|      true     | sparsity | Pass |   10.601624488830566   |\n|      true     | fidelity | Pass | 2.9143777169338136e-07 |\n|     wrong     | sparsity | Pass |   16.99894905090332    |\n|     wrong     | fidelity | Fail |   2.330275297164917    |\n| least_squares | sparsity | Fail |   96.85482025146484    |\n| least_squares | fidelity | Pass | 5.277252626001427e-07  |\n|      pred     | sparsity | Pass |   10.332365989685059   |\n|      pred     | fidelity | Pass | 3.8547517760889605e-05 |\n+---------------+----------+------+------------------------+\n</pre> In\u00a0[7]: Copied! <pre>import os\n\nif not os.path.exists(\"./files/\"):\n    os.makedirs(\"./files/\")\n\n\nmethod_name = ['inf', 'true', 'wrong_sparse', 'least_squares']\nmethod_vec  = [x_pred, x_true, x_wrong, x_ls]\n\nfor i, name in enumerate(method_name):\n    filename = './files/dict_' + name + '.csv'\n    with open(filename, 'w') as f:\n        x  = method_vec[i].detach()\n        Kx = model.K.detach().cpu() @ x\n        for i in range(len(x)):\n            f.write(str(float(Kx[i].numpy())) + '\\n')\n\nwith open('./files/true.csv', 'w') as f:\n    for i in range(len(x_true)):\n        f.write(str(float(x_true[i].numpy())) + '\\n')\n\nimport shutil\nshutil.make_archive('toy-example-files', 'zip', './files/')\n\nfrom google.colab import files\nfiles.download('toy-example-files.zip')\n</pre> import os  if not os.path.exists(\"./files/\"):     os.makedirs(\"./files/\")   method_name = ['inf', 'true', 'wrong_sparse', 'least_squares'] method_vec  = [x_pred, x_true, x_wrong, x_ls]  for i, name in enumerate(method_name):     filename = './files/dict_' + name + '.csv'     with open(filename, 'w') as f:         x  = method_vec[i].detach()         Kx = model.K.detach().cpu() @ x         for i in range(len(x)):             f.write(str(float(Kx[i].numpy())) + '\\n')  with open('./files/true.csv', 'w') as f:     for i in range(len(x_true)):         f.write(str(float(x_true[i].numpy())) + '\\n')  import shutil shutil.make_archive('toy-example-files', 'zip', './files/')  from google.colab import files files.download('toy-example-files.zip') <pre>&lt;ipython-input-7-8d01e7a91935&gt;:16: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  f.write(str(float(Kx[i].numpy())) + '\\n')\n&lt;ipython-input-7-8d01e7a91935&gt;:20: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  f.write(str(float(x_true[i].numpy())) + '\\n')\n</pre>"},{"location":"tutorial/#tutorial","title":"Tutorial\u00b6","text":"<p>Let's consider a toy problem. Here we consider a signal $\\mathsf{x^\\star \\in \\mathbb{R}^n}$ where $\\mathsf{x^\\star = M s}$ with $\\mathsf{M \\in \\mathbb{R}^{n\\times r}}$ and $\\mathsf{s\\in\\mathbb{R}^r}$. The vector $\\mathsf{s}$ is generated as the element-wise product of a Gaussian random vector and a Bernoulli vector ($\\mathsf{p}$ the probability that each entry is nonzero), each vector with i.i.d. entries.</p> <p>Access is given to linear measurements $ \\mathsf{d = A x^\\star}, $ where $\\mathsf{A\\in\\mathbb{R}^{m\\times n}}$ is a matrix with normalized columns (up to numerical tolerance). The task at hand is to</p> <p>$$\\mathsf{ Find\\  x^\\star\\ given\\ d\\ and\\ A}.$$</p> <p>Using the fact that $\\mathsf{p\\cdot r\\ll n}$, we know $\\mathsf{x^\\star}$ admits a sparse representation. Thus, we estimate</p> <p>$$ \\mathsf{x^\\star \\approx argmin_{x} \\ \\|K x\\|_1 \\ \\ \\mbox{s.t.}\\ \\ Ax=d.} $$</p> <p>In this case, the implicit L2O model takes as input $\\mathsf{d}$ and outputs an inference via</p> <p>$$ \\mathsf{{N_{\\theta}}(d) = argmin_{x} \\ \\|K x\\|_1 \\ \\ \\mbox{s.t.}\\ \\ Ax=d}.$$</p> <p>Throughout, we take $\\mathsf{m=100}$, $\\mathsf{n=250}$, $\\mathsf{r=50}$, and $\\mathsf{p=0.1}$.</p>"},{"location":"tutorial/#model-training","title":"Model Training\u00b6","text":"<p>With the model loaded, we next train it to predict $\\mathsf{x^\\star}$ from $\\mathsf{d}$. We use the Adam optimizer and print samples from test data every few epochs to give intuition for how well the parameters are tuned.</p>"},{"location":"tutorial/#property-values","title":"Property Values\u00b6","text":"<p>Below we define measurements for the two properties of signals of interest.</p> <ul> <li>$\\ell_1$ is used as a surrogate for sparsity, i.e., $\\|x\\|_1$</li> <li>Fidelity is a \"pass\" if tolerance $\\|Ax-d\\|\\leq\\epsilon\\cdot \\|A^\\top d\\|$ is satisfied.</li> </ul>"},{"location":"tutorial/#compute-certificate-thresholds","title":"Compute Certificate Thresholds\u00b6","text":"<p>Here's how certificates work.</p> <ol> <li>Compute property value for each sample in training data (using trained L2O model).</li> <li>Decide probability/threshold for pass/warning/fail.</li> <li>Find property value for thresholds of interest.</li> <li>Pass flag function with these threshold values into L2O model.</li> </ol> <p>For simplicity, here we only use \"pass\" and \"fail\" flags.</p>"},{"location":"tutorial/#make-a-histogram-for-sparsity-of-inferences-on-training-data","title":"Make a Histogram for Sparsity of Inferences on Training Data\u00b6","text":"<p>This is used to identify the threshold for where to flag inferences.</p>"},{"location":"tutorial/#summary-graphics","title":"Summary Graphics\u00b6","text":"<p>Below we generate signalx $\\mathsf{x}$ a few different ways and output the corresponding certificates.</p>"},{"location":"tutorial/#save-sample-images-to-file","title":"Save Sample Images to File\u00b6","text":""}]}